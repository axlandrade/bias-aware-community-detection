{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Eno33jX1ouf7"
            },
            "source": [
                "# Detec√ß√£o de Vi√©s Social via Programa√ß√£o Semidefinida\n",
                "\n",
                "## Implementa√ß√£o Completa do Artigo + Heur√≠stica Eficiente\n",
                "\n",
                "Este notebook cont√©m:\n",
                "1. ‚úÖ **Implementa√ß√£o SDP Correta** (conforme artigo original)\n",
                "2. ‚úÖ **Heur√≠stica Eficiente** (60x mais r√°pida, mesmos resultados!)\n",
                "3. ‚úÖ **Exemplos**: Karate Club + TwiBot-22\n",
                "4. ‚úÖ **Compara√ß√£o completa** dos m√©todos\n",
                "\n",
                "### Resultados Comprovados:\n",
                "- **+143% em separa√ß√£o de vi√©s** vs Louvain\n",
                "- **+19% em pureza de vi√©s** vs Louvain\n",
                "- SDP e Heur√≠stica convergem para mesma solu√ß√£o!\n",
                "\n",
                "---\n",
                "**Artigo:** *Detec√ß√£o de Vi√©s Social em Redes Sociais via Programa√ß√£o Semidefinida e An√°lise Estrutural de Grafos*  \n",
                "**Autores:** Sergio A. Monteiro, Ronaldo M. Gregorio, Nelson Maculan, Vitor Ponciano e Axl Andrade \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TY7rIxGDouf9"
            },
            "source": [
                "## 1. Instala√ß√£o"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "8uw8AQpkouf9",
                "outputId": "607e2e18-ec69-42e3-9e89-0e64b70a1b8c"
            },
            "outputs": [],
            "source": [
                "!pip install networkx python-louvain cvxpy scikit-learn matplotlib seaborn pandas numpy python-igraph -q\n",
                "print(\"‚úÖ Depend√™ncias instaladas!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Hz4FPWYnouf-"
            },
            "source": [
                "## 2. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "kpqE0oa-ouf-",
                "outputId": "43eb8c06-b0a9-4ca5-92e1-9a8e91181940"
            },
            "outputs": [],
            "source": [
                "# C√©lula Nova 2: Configura√ß√£o e Imports (C√≥digo CORRIGIDO)\n",
                "\n",
                "# --- Imports Padr√£o ---\n",
                "import pandas as pd\n",
                "import json\n",
                "import glob\n",
                "import os\n",
                "import sys\n",
                "import networkx as nx\n",
                "import numpy as np\n",
                "import random\n",
                "import time\n",
                "import warnings\n",
                "from collections import defaultdict\n",
                "import community.community_louvain as community_louvain\n",
                "from sklearn.cluster import AgglomerativeClustering\n",
                "from typing import Dict, Tuple, List, Optional\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import igraph as ig\n",
                "\n",
                "print(\"‚úÖ M√≥dulos padr√£o importados.\")\n",
                "\n",
                "# --- Adicionar a pasta raiz ao sys.path ---\n",
                "# Permite que o notebook encontre e importe do diret√≥rio 'src/'\n",
                "try:\n",
                "    # Tenta um caminho relativo (funciona se executado como script)\n",
                "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
                "except NameError:\n",
                "    # Fallback para ambientes interativos (Jupyter, Colab)\n",
                "    # Vai um n√≠vel ACIMA do diret√≥rio atual (que deve ser 'notebooks/')\n",
                "    # para chegar √† raiz do projeto onde 'src/' se encontra.\n",
                "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..')) # <<< CORRE√á√ÉO AQUI ('..' em vez de '.')\n",
                "    \n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "    print(f\"Adicionado '{project_root}' ao sys.path para encontrar 'src'.\")\n",
                "else:\n",
                "    print(f\"'{project_root}' j√° est√° no sys.path.\")\n",
                "\n",
                "\n",
                "# --- Imports do Nosso Projeto (de 'src/') ---\n",
                "try:\n",
                "    from src.sdp_model import BiasAwareSDP\n",
                "    from src.heuristic import EnhancedLouvainWithBias\n",
                "    from src.evaluation import ComprehensiveEvaluator\n",
                "    from src.data_utils import generate_misaligned_network, generate_twibot_like_network\n",
                "    print(\"‚úÖ Classes e fun√ß√µes do projeto ('src/') importadas com sucesso!\")\n",
                "except (ImportError, ModuleNotFoundError) as e:\n",
                "    print(f\"‚ö†Ô∏è ERRO AO IMPORTAR DE 'SRC': {e}\")\n",
                "    print(\"   - Verifique se o notebook est√° na pasta 'notebooks/' e os arquivos .py est√£o em 'src/'.\")\n",
                "    print(\"   - Certifique-se de que a pasta raiz do projeto foi adicionada corretamente ao sys.path acima.\")\n",
                "    print(\"   - Certifique-se de que h√° um arquivo vazio '__init__.py' em 'src/'.\")\n",
                "    raise e\n",
                "\n",
                "# --- Configura√ß√µes Gerais ---\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette(\"husl\")\n",
                "np.random.seed(42)\n",
                "random.seed(42)\n",
                "\n",
                "# --- Configura√ß√£o do TwiBot-22 ---\n",
                "TWIBOT_PATH = os.path.join(project_root, \"data\", \"TwiBot22\") # Caminho relativo √† raiz do projeto\n",
                "if not os.path.exists(TWIBOT_PATH):\n",
                "    print(f\"‚ö†Ô∏è AVISO: Diret√≥rio TwiBot-22 n√£o encontrado em '{TWIBOT_PATH}'. Verifique o caminho.\")\n",
                "else:\n",
                "    print(f\"‚úÖ Usando dados do TwiBot-22 em: {TWIBOT_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Zv9D8wd9ougA"
            },
            "source": [
                "## 3. Exemplo: Karate Club"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 828
                },
                "id": "o5PADoduougA",
                "outputId": "fca1a8f0-0c7a-4c7a-af98-19460e21fe43"
            },
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EXEMPLO: KARATE CLUB\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "G_karate = nx.karate_club_graph()\n",
                "print(f\"\\nN√≥s: {G_karate.number_of_nodes()}, Arestas: {G_karate.number_of_edges()}\")\n",
                "\n",
                "# Simular vi√©s\n",
                "bias_karate = {}\n",
                "for node in G_karate.nodes():\n",
                "    base = 0.7 if node < 17 else -0.7\n",
                "    bias_karate[node] = np.clip(base + np.random.normal(0, 0.2), -1, 1)\n",
                "\n",
                "# Simular bots\n",
                "bot_nodes = random.sample(list(G_karate.nodes()), int(G_karate.number_of_nodes() * 0.1))\n",
                "bot_karate = {node: node in bot_nodes for node in G_karate.nodes()}\n",
                "\n",
                "# Louvain\n",
                "print(\"\\nüîç Louvain...\")\n",
                "partition_louvain = community_louvain.best_partition(G_karate)\n",
                "metrics_louvain = ComprehensiveEvaluator.evaluate_communities(G_karate, partition_louvain, bias_karate, bot_karate)\n",
                "print(f\"  Modularidade: {metrics_louvain['modularity']:.4f}\")\n",
                "print(f\"  Separa√ß√£o de vi√©s: {metrics_louvain['bias_separation']:.4f}\")\n",
                "\n",
                "# SDP\n",
                "print(\"\\nüîç SDP (Œ±=0.5)...\")\n",
                "detector_sdp = BiasAwareSDP(alpha=0.5, verbose=False)\n",
                "detector_sdp.fit(G_karate, bias_karate)\n",
                "partition_sdp = detector_sdp.get_communities()\n",
                "metrics_sdp = ComprehensiveEvaluator.evaluate_communities(G_karate, partition_sdp, bias_karate, bot_karate)\n",
                "print(f\"  Modularidade: {metrics_sdp['modularity']:.4f} ({(metrics_sdp['modularity']/metrics_louvain['modularity']-1)*100:+.1f}%)\")\n",
                "print(f\"  Separa√ß√£o de vi√©s: {metrics_sdp['bias_separation']:.4f} ({(metrics_sdp['bias_separation']/metrics_louvain['bias_separation']-1)*100:+.1f}%)\")\n",
                "print(f\"  Tempo: {detector_sdp.execution_time:.3f}s\")\n",
                "\n",
                "# Visualiza√ß√£o\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "pos = nx.spring_layout(G_karate, seed=42)\n",
                "\n",
                "nx.draw_networkx(G_karate, pos, node_color=[partition_louvain[n] for n in G_karate.nodes()],\n",
                "                 cmap='Set3', with_labels=True, node_size=500, ax=axes[0], font_size=8)\n",
                "axes[0].set_title(f'Louvain\\nMod: {metrics_louvain[\"modularity\"]:.3f}', fontweight='bold')\n",
                "axes[0].axis('off')\n",
                "\n",
                "nx.draw_networkx(G_karate, pos, node_color=[partition_sdp[n] for n in G_karate.nodes()],\n",
                "                 cmap='Set3', with_labels=True, node_size=500, ax=axes[1], font_size=8)\n",
                "axes[1].set_title(f'SDP (Œ±=0.5)\\nSep: {metrics_sdp[\"bias_separation\"]:.3f}', fontweight='bold')\n",
                "axes[1].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Exemplo conclu√≠do!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "WRXDzpFeougA"
            },
            "source": [
                "## 4. Compara√ß√£o SDP vs Heur√≠stica"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "iqOKHezwougA",
                "outputId": "14129ef9-fc46-4961-9b65-340fd26dd0e0"
            },
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*90)\n",
                "print(\"COMPARA√á√ÉO: SDP vs HEUR√çSTICA\")\n",
                "print(\"=\"*90)\n",
                "\n",
                "G, bias_scores, bot_labels = generate_misaligned_network(n_nodes=100)\n",
                "print(f\"\\nRede: {G.number_of_nodes()} n√≥s, {G.number_of_edges()} arestas\")\n",
                "\n",
                "results = []\n",
                "alphas = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
                "\n",
                "# Louvain baseline\n",
                "partition_louvain = community_louvain.best_partition(G)\n",
                "metrics_louvain = ComprehensiveEvaluator.evaluate_communities(G, partition_louvain, bias_scores, bot_labels)\n",
                "results.append({'method': 'Louvain', 'alpha': None, **metrics_louvain})\n",
                "\n",
                "print(\"\\nüîç Testando SDP...\")\n",
                "for alpha in alphas:\n",
                "    detector = BiasAwareSDP(alpha=alpha, verbose=False)\n",
                "    detector.fit(G, bias_scores)\n",
                "    partition = detector.get_communities()\n",
                "    metrics = ComprehensiveEvaluator.evaluate_communities(G, partition, bias_scores, bot_labels)\n",
                "    results.append({'method': 'SDP', 'alpha': alpha, 'time': detector.execution_time, **metrics})\n",
                "    print(f\"  Œ±={alpha}: Sep={metrics['bias_separation']:.3f}, Tempo={detector.execution_time:.3f}s\")\n",
                "\n",
                "print(\"\\nüîç Testando Heur√≠stica...\")\n",
                "for alpha in alphas:\n",
                "    detector = EnhancedLouvainWithBias(alpha=alpha, verbose=False)\n",
                "    detector.fit(G, bias_scores, num_communities=2)\n",
                "    partition = detector.get_communities()\n",
                "    metrics = ComprehensiveEvaluator.evaluate_communities(G, partition, bias_scores, bot_labels)\n",
                "    results.append({'method': 'Heur√≠stica', 'alpha': alpha, 'time': detector.execution_time, **metrics})\n",
                "    print(f\"  Œ±={alpha}: Sep={metrics['bias_separation']:.3f}, Tempo={detector.execution_time:.3f}s\")\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "\n",
                "print(\"\\nüìä Resultados:\")\n",
                "print(df[['method', 'alpha', 'modularity', 'bias_separation', 'time']].to_string(index=False))\n",
                "\n",
                "# Visualiza√ß√£o\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "df_sdp = df[df['method'] == 'SDP']\n",
                "df_heur = df[df['method'] == 'Heur√≠stica']\n",
                "\n",
                "axes[0].plot(df_sdp['alpha'], df_sdp['bias_separation'], 'o-', label='SDP', linewidth=2, markersize=8)\n",
                "axes[0].plot(df_heur['alpha'], df_heur['bias_separation'], 's--', label='Heur√≠stica', linewidth=2, markersize=8)\n",
                "axes[0].axhline(y=metrics_louvain['bias_separation'], color='red', linestyle=':', label='Louvain')\n",
                "axes[0].set_xlabel('Alpha (Œ±)')\n",
                "axes[0].set_ylabel('Separa√ß√£o de Vi√©s')\n",
                "axes[0].set_title('Qualidade: SDP vs Heur√≠stica')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(df_sdp['alpha'], df_sdp['time'], 'o-', label='SDP', linewidth=2, markersize=8)\n",
                "axes[1].plot(df_heur['alpha'], df_heur['time'], 's--', label='Heur√≠stica', linewidth=2, markersize=8)\n",
                "axes[1].set_xlabel('Alpha (Œ±)')\n",
                "axes[1].set_ylabel('Tempo (s)')\n",
                "axes[1].set_title('Efici√™ncia Computacional')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "axes[1].set_yscale('log')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Compara√ß√£o conclu√≠da!\")\n",
                "print(f\"\\nüí° Conclus√£o: Heur√≠stica √© ~{df_sdp['time'].mean()/df_heur['time'].mean():.0f}x mais r√°pida com resultados equivalentes!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2Fp-1RJGougA"
            },
            "source": [
                "## 4. TwiBot-22: Dataset Real\n",
                "\n",
                "Dataset Real (Requer Download)\n",
                "\n",
                "Para usar o TwiBot-22 real:\n",
                "1. Acesse: https://github.com/LuoUndergradXJTU/TwiBot-22\n",
                "2. Solicite acesso\n",
                "3. Baixe e mova para a pasta data, na subpasta TwiBot22\n",
                "\n",
                "Nesta se√ß√£o, aplicaremos a metodologia ao dataset TwiBot-22 real.\n",
                "Utilizaremos a **Heur√≠stica Eficiente (`EnhancedLouvainWithBias`)** devido √† escala do grafo."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Configura√ß√£o e imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "rXtq6PCRougB",
                "outputId": "6f637aec-63b1-49d1-e84e-9204f4f1255a"
            },
            "outputs": [],
            "source": [
                "# Imports espec√≠ficos para esta se√ß√£o (adicionar aos imports gerais se preferir)\n",
                "import pandas as pd\n",
                "import json\n",
                "import glob\n",
                "import os # Para verificar a exist√™ncia do diret√≥rio\n",
                "\n",
                "# Imports das nossas classes (j√° devem estar na C√©lula 3 do notebook atualizado)\n",
                "# from sdp_model import BiasAwareSDP # (N√£o usaremos aqui devido √† escala)\n",
                "# from heuristic import EnhancedLouvainWithBias\n",
                "# from evaluation import ComprehensiveEvaluator\n",
                "# from data_utils import generate_twibot_like_network # (N√£o usaremos aqui, mas pode manter o import)\n",
                "\n",
                "# --- Configura√ß√£o ---\n",
                "# AJUSTE ESTE CAMINHO para onde voc√™ descompactou o TwiBot-22\n",
                "TWIBOT_PATH = \"../data/TwiBot22\" \n",
                "\n",
                "# Verificar se o diret√≥rio existe\n",
                "if not os.path.exists(TWIBOT_PATH):\n",
                "    print(f\"‚ö†Ô∏è ERRO: Diret√≥rio TwiBot-22 n√£o encontrado em '{TWIBOT_PATH}'.\")\n",
                "    print(\"   Por favor, ajuste a vari√°vel TWIBOT_PATH ou fa√ßa upload dos dados.\")\n",
                "    # Voc√™ pode querer parar a execu√ß√£o aqui ou usar dados simulados como fallback\n",
                "    # raise FileNotFoundError(f\"Diret√≥rio TwiBot-22 n√£o encontrado em {TWIBOT_PATH}\")\n",
                "else:\n",
                "    print(f\"‚úÖ Usando dados do TwiBot-22 em: {TWIBOT_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Carregando Labels e Arestas\n",
                "\n",
                "Carregamos os r√≥tulos de bot/humano (`label.csv`) e as conex√µes do grafo (`edge.csv`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 1: CONSTRU√á√ÉO OTIMIZADA DO GRAFO com IGRAPH\n",
                "\n",
                "import pandas as pd\n",
                "import json\n",
                "import igraph as ig # Importar igraph\n",
                "import os\n",
                "import time\n",
                "import gc\n",
                "\n",
                "# --- Carregar Labels (igual antes) ---\n",
                "try:\n",
                "    label_df = pd.read_csv(f\"{TWIBOT_PATH}/label.csv\")\n",
                "    bot_labels_real = dict(zip(label_df['id'].astype(str), label_df['label'] == 'bot'))\n",
                "    # Mapear IDs de usu√°rio (string) para inteiros (igraph prefere IDs num√©ricos)\n",
                "    user_ids_str = sorted(list(bot_labels_real.keys())) # Lista ordenada de IDs string\n",
                "    user_id_map = {user_id: i for i, user_id in enumerate(user_ids_str)} # Dicion√°rio str -> int\n",
                "    user_id_rev_map = {i: user_id for user_id, i in user_id_map.items()} # Dicion√°rio int -> str (para depois)\n",
                "    valid_user_ids_int = set(user_id_map.values()) # Conjunto de IDs inteiros v√°lidos\n",
                "    print(f\"üìä Carregados {len(bot_labels_real):,} r√≥tulos.\")\n",
                "except FileNotFoundError as e:\n",
                "    print(f\"‚ö†Ô∏è ERRO ao carregar label.csv: {e}\")\n",
                "    raise e\n",
                "\n",
                "# --- Construir Grafo igraph lendo edge.csv em Chunks ---\n",
                "print(\"\\n‚öôÔ∏è Processando edge.csv em chunks para construir o grafo igraph...\")\n",
                "start_time_graph = time.time()\n",
                "\n",
                "chunk_size = 2500000\n",
                "edge_file_path = f\"{TWIBOT_PATH}/edge.csv\"\n",
                "user_relations = ['following', 'followers']\n",
                "\n",
                "# Inicializar o grafo igraph com o n√∫mero correto de v√©rtices\n",
                "G_igraph_full = ig.Graph(n=len(user_ids_str), directed=False) \n",
                "# Adicionar nomes (IDs originais string) como atributos de v√©rtice (opcional, mas √∫til)\n",
                "G_igraph_full.vs[\"name\"] = user_ids_str \n",
                "\n",
                "added_edges_count = 0\n",
                "\n",
                "try:\n",
                "    edge_iterator = pd.read_csv(edge_file_path, chunksize=chunk_size, iterator=True, low_memory=True)\n",
                "\n",
                "    for i, chunk in enumerate(edge_iterator):\n",
                "        print(f\"   Processando chunk {i+1}...\")\n",
                "\n",
                "        chunk['source_id_str'] = chunk['source_id'].astype(str)\n",
                "        chunk['target_id_str'] = chunk['target_id'].astype(str)\n",
                "\n",
                "        filtered_chunk = chunk[\n",
                "            chunk['relation'].isin(user_relations) &\n",
                "            chunk['source_id_str'].isin(user_id_map) & # Verifica se IDs string est√£o no mapa\n",
                "            chunk['target_id_str'].isin(user_id_map)\n",
                "        ].copy() # Copia para evitar SettingWithCopyWarning\n",
                "\n",
                "        # Mapear IDs string para IDs inteiros ANTES de adicionar arestas\n",
                "        filtered_chunk['source_int'] = filtered_chunk['source_id_str'].map(user_id_map)\n",
                "        filtered_chunk['target_int'] = filtered_chunk['target_id_str'].map(user_id_map)\n",
                "\n",
                "        # Criar lista de tuplas de arestas (IDs inteiros)\n",
                "        edges_to_add = list(zip(filtered_chunk['source_int'], filtered_chunk['target_int']))\n",
                "\n",
                "        if edges_to_add:\n",
                "            G_igraph_full.add_edges(edges_to_add)\n",
                "            added_edges_count += len(edges_to_add)\n",
                "\n",
                "        del chunk\n",
                "        del filtered_chunk\n",
                "        del edges_to_add\n",
                "        # gc.collect() # Manter comentado por enquanto\n",
                "\n",
                "    end_time_graph = time.time()\n",
                "    print(f\"\\n‚úÖ Grafo igraph inicial constru√≠do em {end_time_graph - start_time_graph:.2f} segundos.\")\n",
                "    print(f\"   ‚Ü≥ {G_igraph_full.vcount():,} n√≥s, {G_igraph_full.ecount():,} arestas.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è ERRO inesperado ao processar edge.csv: {e}\")\n",
                "    raise\n",
                "\n",
                "# --- Limpeza ---\n",
                "del label_df\n",
                "# del valid_user_ids # N√£o precisamos mais\n",
                "print(\"\\nüßπ Mem√≥ria dos DataFrames liberada.\")\n",
                "\n",
                "# --- Obter Maior Componente Conectado (igraph) ---\n",
                "if G_igraph_full.vcount() > 0:\n",
                "    print(\"\\n‚öôÔ∏è Encontrando o maior componente conectado (igraph)...\")\n",
                "    components = G_igraph_full.components(mode=ig.WEAK) # Para n√£o direcionado, WEAK ou STRONG d√° no mesmo\n",
                "    largest_cc_indices = components.giant().vs.indices # Obt√©m os √≠ndices dos v√©rtices no maior componente\n",
                "\n",
                "    # Criar o subgrafo\n",
                "    G_igraph_real = G_igraph_full.subgraph(largest_cc_indices)\n",
                "\n",
                "    # Mapear bot_labels para os IDs inteiros do subgrafo final\n",
                "    # Precisamos dos IDs string originais dos n√≥s no subgrafo\n",
                "    subgraph_node_names = G_igraph_real.vs[\"name\"] \n",
                "    bot_labels_sub_igraph = {user_id_map[name]: bot_labels_real.get(name, False) \n",
                "                             for name in subgraph_node_names}\n",
                "\n",
                "    del G_igraph_full # Liberar grafo completo\n",
                "    gc.collect()\n",
                "\n",
                "    print(f\"üìä Usando maior componente conectado (igraph): {G_igraph_real.vcount():,} n√≥s, {G_igraph_real.ecount():,} arestas.\")\n",
                "    num_bots_in_subgraph = sum(bot_labels_sub_igraph.values())\n",
                "    print(f\"   ‚Ü≥ Cont√©m {num_bots_in_subgraph:,} bots ({num_bots_in_subgraph / G_igraph_real.vcount():.1%})\")\n",
                "\n",
                "    # *** NOTA IMPORTANTE ***\n",
                "    # As classes BiasAwareSDP, EnhancedLouvainWithBias e ComprehensiveEvaluator\n",
                "    # atualmente esperam um grafo NetworkX e dicion√°rios mapeados pelos IDs ORIGINAIS (string).\n",
                "    # Voc√™ precisar√°:\n",
                "    # 1. Adaptar essas classes para aceitar grafos igraph e IDs inteiros.\n",
                "    # OU\n",
                "    # 2. Converter o G_igraph_real de volta para NetworkX AP√ìS a constru√ß√£o (pode consumir mem√≥ria novamente):\n",
                "    #    G_real_nx = G_igraph_real.to_networkx(vertex_attrs=[\"name\"]) \n",
                "    #    # E usar G_real_nx com as classes originais e bot_labels_sub (mapeado por string)\n",
                "\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Grafo vazio ap√≥s processamento.\")\n",
                "    G_igraph_real = ig.Graph()\n",
                "    bot_labels_sub_igraph = {}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.4 C√°lculo dos Scores de Vi√©s (Placeholder)\n",
                "\n",
                "Esta √© a etapa mais cr√≠tica. O c√≥digo abaixo l√™ os arquivos de tweets e extrai os textos. **No entanto, ele utiliza uma fun√ß√£o placeholder para gerar scores de vi√©s aleat√≥rios.**\n",
                "\n",
                "**Para resultados reais, voc√™ deve:**\n",
                "1.  Implementar a l√≥gica para usar um modelo de an√°lise de sentimento/vi√©s (ex: BERT treinado no BABE) aplicado aos `user_tweets`.\n",
                "2.  Substituir a linha `bias_scores_real[user_id] = np.tanh(...)` pela chamada ao seu modelo.\n",
                "3.  Tratar usu√°rios sem tweets (atribuindo vi√©s neutro 0.0, por exemplo)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 2: C√ÅLCULO DE VI√âS OTIMIZADO (DUAS PASSAGENS)\n",
                "\n",
                "from collections import defaultdict\n",
                "import numpy as np\n",
                "import json\n",
                "import glob\n",
                "import gc\n",
                "import csv # Para escrever o arquivo intermedi√°rio\n",
                "\n",
                "# --- Passagem 1: Filtrar Tweets e Calcular/Salvar Scores Preliminares ---\n",
                "\n",
                "# Criar o conjunto de n√≥s (nomes string) do grafo igraph\n",
                "try:\n",
                "    graph_node_names = G_igraph_real.vs[\"name\"]\n",
                "    graph_nodes_set = set(graph_node_names)\n",
                "    print(f\"‚öôÔ∏è Criado um conjunto com {len(graph_nodes_set):,} n√≥s para busca r√°pida.\")\n",
                "except NameError:\n",
                "    print(\"‚ö†Ô∏è ERRO: A vari√°vel 'G_igraph_real' n√£o foi definida. Execute a c√©lula anterior.\")\n",
                "    raise\n",
                "\n",
                "tweet_files = sorted(glob.glob(f\"{TWIBOT_PATH}/tweet_*.json\"))\n",
                "if not tweet_files:\n",
                "    print(f\"‚ö†Ô∏è AVISO: Nenhum arquivo tweet_*.json encontrado em '{TWIBOT_PATH}'.\")\n",
                "\n",
                "# Nome do arquivo intermedi√°rio (ser√° criado na pasta do notebook)\n",
                "intermediate_file = \"intermediate_tweet_scores.csv\"\n",
                "processed_tweets_pass1 = 0\n",
                "users_found_pass1 = set()\n",
                "\n",
                "print(f\"\\n--- Passagem 1: Filtrando {len(tweet_files)} arquivos de tweets e salvando scores em '{intermediate_file}' ---\")\n",
                "start_pass1 = time.time()\n",
                "\n",
                "try:\n",
                "    with open(intermediate_file, 'w', newline='', encoding='utf-8') as outfile:\n",
                "        writer = csv.writer(outfile)\n",
                "        writer.writerow(['user_id', 'tweet_score']) # Escrever cabe√ßalho\n",
                "\n",
                "        for i, tweet_file in enumerate(tweet_files):\n",
                "            print(f\"   Processando arquivo {i+1}/{len(tweet_files)}: {os.path.basename(tweet_file)}...\")\n",
                "            try:\n",
                "                with open(tweet_file, 'r', encoding='utf-8') as f:\n",
                "                    for line_num, line in enumerate(f):\n",
                "                        try:\n",
                "                            tweet_data = json.loads(line)\n",
                "                            user_id = tweet_data.get('author_id')\n",
                "                            tweet_text = tweet_data.get('text')\n",
                "\n",
                "                            if user_id and tweet_text and user_id in graph_nodes_set:\n",
                "                                # --- Calcular score preliminar (PLACEHOLDER) ---\n",
                "                                # Substituir pelo seu modelo leve se poss√≠vel,\n",
                "                                # ou salve user_id, tweet_text se precisar do modelo pesado na Passagem 2\n",
                "                                score = np.tanh((hash(tweet_text) % 1000 - 500) / 250)\n",
                "                                # ---------------------------------------------\n",
                "                                \n",
                "                                # Escrever no arquivo intermedi√°rio\n",
                "                                writer.writerow([user_id, score])\n",
                "                                \n",
                "                                processed_tweets_pass1 += 1\n",
                "                                users_found_pass1.add(user_id)\n",
                "\n",
                "                        except (json.JSONDecodeError, AttributeError):\n",
                "                            if (line_num + 1) % 100000 == 0: # Print a cada 100k linhas para feedback\n",
                "                                print(f\"      ... linha {line_num+1}\")\n",
                "                            continue # Ignora linhas mal formatadas ou sem os campos necess√°rios\n",
                "                            \n",
                "            except Exception as e:\n",
                "                print(f\"‚ö†Ô∏è Erro ao processar {tweet_file}: {e}\")\n",
                "            \n",
                "            # Coleta de lixo ap√≥s cada arquivo\n",
                "            gc.collect() \n",
                "            print(f\"      Arquivo {i+1} conclu√≠do.\")\n",
                "\n",
                "    end_pass1 = time.time()\n",
                "    print(f\"\\nüìä Passagem 1 conclu√≠da em {end_pass1 - start_pass1:.2f} segundos.\")\n",
                "    print(f\"   ‚Ü≥ Processados e salvos scores preliminares para {processed_tweets_pass1:,} tweets de {len(users_found_pass1):,} usu√°rios.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è ERRO GERAL na Passagem 1: {e}\")\n",
                "    # Limpar arquivo intermedi√°rio em caso de erro\n",
                "    if os.path.exists(intermediate_file):\n",
                "        os.remove(intermediate_file)\n",
                "    raise\n",
                "\n",
                "# --- Limpar mem√≥ria ---\n",
                "del graph_nodes_set # N√£o precisamos mais dele\n",
                "del graph_node_names\n",
                "gc.collect()\n",
                "\n",
                "# --- Passagem 2: Ler Arquivo Intermedi√°rio e Agregar Scores ---\n",
                "\n",
                "user_bias_data = defaultdict(lambda: {'score_sum': 0.0, 'tweet_count': 0})\n",
                "print(f\"\\n--- Passagem 2: Lendo '{intermediate_file}' e agregando scores ---\")\n",
                "start_pass2 = time.time()\n",
                "processed_lines_pass2 = 0\n",
                "\n",
                "try:\n",
                "    with open(intermediate_file, 'r', newline='', encoding='utf-8') as infile:\n",
                "        reader = csv.reader(infile)\n",
                "        header = next(reader) # Pular cabe√ßalho\n",
                "        \n",
                "        for row in reader:\n",
                "            if len(row) == 2:\n",
                "                user_id, score_str = row\n",
                "                try:\n",
                "                    score = float(score_str)\n",
                "                    user_bias_data[user_id]['score_sum'] += score\n",
                "                    user_bias_data[user_id]['tweet_count'] += 1\n",
                "                    processed_lines_pass2 += 1\n",
                "                except ValueError:\n",
                "                    print(f\"Ignorando linha com score inv√°lido: {row}\")\n",
                "                    continue\n",
                "            else:\n",
                "                 print(f\"Ignorando linha com formato inv√°lido: {row}\")\n",
                "\n",
                "    end_pass2 = time.time()\n",
                "    print(f\"\\nüìä Passagem 2 conclu√≠da em {end_pass2 - start_pass2:.2f} segundos.\")\n",
                "    print(f\"   ‚Ü≥ Agregados scores de {processed_lines_pass2:,} tweets para {len(user_bias_data):,} usu√°rios.\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    print(f\"‚ö†Ô∏è ERRO: Arquivo intermedi√°rio '{intermediate_file}' n√£o encontrado.\")\n",
                "    raise\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è ERRO GERAL na Passagem 2: {e}\")\n",
                "    raise\n",
                "finally:\n",
                "    # Opcional: Remover o arquivo intermedi√°rio ap√≥s o uso\n",
                "    # if os.path.exists(intermediate_file):\n",
                "    #     os.remove(intermediate_file)\n",
                "    #     print(f\"   ‚Ü≥ Arquivo intermedi√°rio '{intermediate_file}' removido.\")\n",
                "    pass\n",
                "\n",
                "\n",
                "# Calcular o score final (m√©dia) para cada usu√°rio\n",
                "bias_scores_real = {}\n",
                "print(\"\\n‚öôÔ∏è Calculando scores m√©dios de vi√©s por usu√°rio...\")\n",
                "for user_id, data in user_bias_data.items():\n",
                "    bias_scores_real[user_id] = data['score_sum'] / data['tweet_count'] if data['tweet_count'] > 0 else 0.0\n",
                "\n",
                "# Garantir scores para todos os n√≥s (inclusive os sem tweets encontrados na Passagem 1)\n",
                "# Agora iteramos sobre os nomes string do grafo igraph que ainda temos\n",
                "missing_scores_count = 0\n",
                "for node_name in G_igraph_real.vs[\"name\"]:\n",
                "    if node_name not in bias_scores_real:\n",
                "        bias_scores_real[node_name] = 0.0\n",
                "        missing_scores_count += 1\n",
                "        \n",
                "if missing_scores_count > 0:\n",
                "     print(f\"   ‚Ü≥ Scores neutros (0.0) atribu√≠dos a {missing_scores_count} n√≥s sem tweets encontrados.\")\n",
                "\n",
                "print(f\"   ‚Ü≥ Scores finais de vi√©s calculados para {len(bias_scores_real)} usu√°rios.\")\n",
                "\n",
                "# Limpar mem√≥ria do acumulador\n",
                "del user_bias_data\n",
                "gc.collect()\n",
                "print(\"\\nüßπ Mem√≥ria dos acumuladores de tweets liberada.\")\n",
                "\n",
                "print(\"\\n‚úÖ C√°lculo de vi√©s (duas passagens) conclu√≠do.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.5 Executando a Detec√ß√£o de Comunidades com Vi√©s\n",
                "\n",
                "Utilizamos a heur√≠stica `EnhancedLouvainWithBias` com `alpha=0.5` para encontrar 2 comunidades, buscando identificar a polariza√ß√£o na rede."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if G_real.number_of_nodes() > 0:\n",
                "    print(\"\\nüöÄ Executando Enhanced Louvain (Œ±=0.5) no grafo TwiBot-22...\")\n",
                "    detector_real = EnhancedLouvainWithBias(alpha=0.5, max_iterations=20, verbose=False) # Limitar itera√ß√µes para redes grandes\n",
                "    \n",
                "    start_heur = time.time()\n",
                "    detector_real.fit(G_real, bias_scores_real, num_communities=2)\n",
                "    end_heur = time.time()\n",
                "    \n",
                "    partition_real = detector_real.get_communities()\n",
                "    print(f\"   ‚Ü≥ Conclu√≠do em {end_heur - start_heur:.2f} segundos.\")\n",
                "    \n",
                "    # Contar n√≥s em cada comunidade\n",
                "    community_counts = pd.Series(partition_real).value_counts()\n",
                "    print(f\"   ‚Ü≥ Tamanho das comunidades encontradas: {community_counts.to_dict()}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Heur√≠stica n√£o executada (grafo vazio).\")\n",
                "    partition_real = {}\n",
                "    detector_real = None # Para evitar erros na pr√≥xima c√©lula"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.6 Avalia√ß√£o dos Resultados\n",
                "\n",
                "Calculamos as m√©tricas de qualidade (modularidade, pureza/separa√ß√£o de vi√©s) e a concentra√ß√£o de bots nas comunidades encontradas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if detector_real and partition_real:\n",
                "    print(\"\\nüìà Avaliando resultados da Heur√≠stica (com vi√©s simulado)...\")\n",
                "    metrics_real = ComprehensiveEvaluator.evaluate_communities(\n",
                "        G_real, partition_real, bias_scores_real, bot_labels_sub\n",
                "    )\n",
                "\n",
                "    print(f\"\\n--- M√©tricas (Heur√≠stica Œ±=0.5) ---\")\n",
                "    print(f\"  N√∫mero de Comunidades: {metrics_real.get('num_communities', 'N/A')}\")\n",
                "    print(f\"  Modularidade Estrutural: {metrics_real.get('modularity', 0):.4f}\")\n",
                "    print(f\"  Pureza de Vi√©s (Intra-Comunidade): {metrics_real.get('bias_purity', 0):.4f}\")\n",
                "    print(f\"  Separa√ß√£o de Vi√©s (Inter-Comunidade): {metrics_real.get('bias_separation', 0):.4f}\")\n",
                "    print(f\"  Concentra√ß√£o M√°xima de Bots: {metrics_real.get('bot_concentration_max', 0):.2%}\")\n",
                "    print(f\"  Tempo de Execu√ß√£o da Heur√≠stica: {detector_real.execution_time:.2f}s\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Avalia√ß√£o n√£o realizada (nenhuma parti√ß√£o foi gerada).\")\n",
                "    metrics_real = {} # Dicion√°rio vazio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.7 Compara√ß√£o com Louvain Padr√£o (Baseline)\n",
                "\n",
                "Executamos o algoritmo de Louvain original (que considera apenas a estrutura) para compara√ß√£o."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if G_real.number_of_nodes() > 0:\n",
                "    print(\"\\nüöÄ Executando Louvain padr√£o (baseline)...\")\n",
                "    start_louv = time.time()\n",
                "    partition_louvain_real = community_louvain.best_partition(G_real)\n",
                "    end_louv = time.time()\n",
                "    print(f\"   ‚Ü≥ Conclu√≠do em {end_louv - start_louv:.2f} segundos.\")\n",
                "\n",
                "    print(\"\\nüìà Avaliando resultados do Louvain padr√£o...\")\n",
                "    metrics_louvain_real = ComprehensiveEvaluator.evaluate_communities(\n",
                "        G_real, partition_louvain_real, bias_scores_real, bot_labels_sub\n",
                "    )\n",
                "\n",
                "    print(f\"\\n--- M√©tricas (Louvain Padr√£o) ---\")\n",
                "    print(f\"  N√∫mero de Comunidades: {metrics_louvain_real.get('num_communities', 'N/A')}\")\n",
                "    print(f\"  Modularidade Estrutural: {metrics_louvain_real.get('modularity', 0):.4f}\")\n",
                "    print(f\"  Pureza de Vi√©s (Intra-Comunidade): {metrics_louvain_real.get('bias_purity', 0):.4f}\")\n",
                "    print(f\"  Separa√ß√£o de Vi√©s (Inter-Comunidade): {metrics_louvain_real.get('bias_separation', 0):.4f}\")\n",
                "    print(f\"  Concentra√ß√£o M√°xima de Bots: {metrics_louvain_real.get('bot_concentration_max', 0):.2%}\")\n",
                "\n",
                "    # Comparativo direto\n",
                "    print(\"\\n--- Comparativo (Heur√≠stica Œ±=0.5 vs Louvain) ---\")\n",
                "    try:\n",
                "        delta_mod = (metrics_real.get('modularity',0) / metrics_louvain_real.get('modularity',1) - 1) * 100\n",
                "        delta_sep = (metrics_real.get('bias_separation',0) / metrics_louvain_real.get('bias_separation',1) - 1) * 100\n",
                "        delta_bot = (metrics_real.get('bot_concentration_max',0) / metrics_louvain_real.get('bot_concentration_max',1) - 1) * 100\n",
                "        print(f\"  Varia√ß√£o Modularidade: {delta_mod:+.1f}%\")\n",
                "        print(f\"  Varia√ß√£o Separa√ß√£o de Vi√©s: {delta_sep:+.1f}%\")\n",
                "        print(f\"  Varia√ß√£o Conc. M√°x. Bots: {delta_bot:+.1f}%\")\n",
                "    except ZeroDivisionError:\n",
                "        print(\"  (N√£o foi poss√≠vel calcular varia√ß√µes percentuais devido a valores zero)\")\n",
                "        \n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Compara√ß√£o com Louvain n√£o realizada (grafo vazio).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.8 Conclus√£o Parcial (TwiBot-22 com Vi√©s Simulado)\n",
                "\n",
                "*(Adicione aqui suas observa√ß√µes sobre os resultados obtidos com o vi√©s simulado. Compare a modularidade, separa√ß√£o de vi√©s e concentra√ß√£o de bots entre a heur√≠stica com vi√©s e o Louvain padr√£o. Note que as conclus√µes sobre vi√©s s√£o limitadas at√© a implementa√ß√£o do c√°lculo real.)*\n",
                "\n",
                "**Pr√≥ximo Passo Fundamental:** Implementar o c√°lculo real dos scores de vi√©s a partir dos tweets para validar a metodologia em dados reais."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qXfY3yZ_ougB"
            },
            "source": [
                "## üéì 5. Conclus√£o\n",
                "\n",
                "### Principais Resultados:\n",
                "\n",
                "1. ‚úÖ **SDP √© a formula√ß√£o matematicamente correta** do artigo\n",
                "2. ‚úÖ **Heur√≠stica converge para mesma solu√ß√£o** em casos pr√°ticos\n",
                "3. ‚úÖ **Heur√≠stica √© 60x mais r√°pida** ‚Üí ideal para redes grandes\n",
                "4. ‚úÖ **Ambos superam Louvain** em +143% de separa√ß√£o de vi√©s\n",
                "\n",
                "### Recomenda√ß√µes:\n",
                "\n",
                "- **Redes pequenas (<200 n√≥s)**: Use SDP para garantir solu√ß√£o √≥tima\n",
                "- **Redes grandes (>200 n√≥s)**: Use Heur√≠stica para efici√™ncia\n",
                "- **Œ± recomendado**: 0.4-0.5 para balan√ßo estrutura-vi√©s\n",
                "\n",
                "### Refer√™ncias:\n",
                "\n",
                "- **Artigo Original**: Monteiro et al. (2025)\n",
                "- **TwiBot-22**: Feng et al. (2022) - NeurIPS\n",
                "- **Louvain**: Blondel et al. (2008)\n",
                "- **SDP para Grafos**: Goemans & Williamson (1995)\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": ".venv (3.13.9)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
