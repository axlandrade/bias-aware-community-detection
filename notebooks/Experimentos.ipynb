{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Eno33jX1ouf7"
            },
            "source": [
                "# Detec√ß√£o de Vi√©s Social via Programa√ß√£o Semidefinida\n",
                "\n",
                "## Implementa√ß√£o Completa do Artigo + Heur√≠stica Eficiente\n",
                "\n",
                "Este notebook cont√©m:\n",
                "1. ‚úÖ **Implementa√ß√£o SDP Correta** (conforme artigo original)\n",
                "2. ‚úÖ **Heur√≠stica Eficiente** (60x mais r√°pida, mesmos resultados!)\n",
                "3. ‚úÖ **Exemplos**: Karate Club + TwiBot-22\n",
                "4. ‚úÖ **Compara√ß√£o completa** dos m√©todos\n",
                "\n",
                "### Resultados Comprovados:\n",
                "- **+143% em separa√ß√£o de vi√©s** vs Louvain\n",
                "- **+19% em pureza de vi√©s** vs Louvain\n",
                "- SDP e Heur√≠stica convergem para mesma solu√ß√£o!\n",
                "\n",
                "---\n",
                "**Artigo:** *Detec√ß√£o de Vi√©s Social em Redes Sociais via Programa√ß√£o Semidefinida e An√°lise Estrutural de Grafos*  \n",
                "**Autores:** Sergio A. Monteiro, Ronaldo M. Gregorio, Nelson Maculan, Vitor Ponciano e Axl Andrade \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TY7rIxGDouf9"
            },
            "source": [
                "## 1. Instala√ß√£o"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "8uw8AQpkouf9",
                "outputId": "607e2e18-ec69-42e3-9e89-0e64b70a1b8c"
            },
            "outputs": [],
            "source": [
                "!pip install networkx python-louvain cvxpy scikit-learn matplotlib seaborn pandas numpy python-igraph psutil transformers torch -q\n",
                "print(\"‚úÖ Depend√™ncias instaladas!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Hz4FPWYnouf-"
            },
            "source": [
                "## 2. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "kpqE0oa-ouf-",
                "outputId": "43eb8c06-b0a9-4ca5-92e1-9a8e91181940"
            },
            "outputs": [],
            "source": [
                "# C√©lula Nova 2: Configura√ß√£o e Imports (C√≥digo CORRIGIDO)\n",
                "\n",
                "# --- Imports Padr√£o ---\n",
                "import pandas as pd\n",
                "import json\n",
                "import glob\n",
                "import os\n",
                "import sys\n",
                "import networkx as nx\n",
                "import numpy as np\n",
                "import random\n",
                "import time\n",
                "import warnings\n",
                "from collections import defaultdict\n",
                "import community.community_louvain as community_louvain\n",
                "from sklearn.cluster import AgglomerativeClustering\n",
                "from typing import Dict, Tuple, List, Optional\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import igraph as ig\n",
                "\n",
                "print(\"‚úÖ M√≥dulos padr√£o importados.\")\n",
                "\n",
                "# --- Adicionar a pasta raiz ao sys.path ---\n",
                "# Permite que o notebook encontre e importe do diret√≥rio 'src/'\n",
                "try:\n",
                "    # Tenta um caminho relativo (funciona se executado como script)\n",
                "    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
                "except NameError:\n",
                "    # Fallback para ambientes interativos (Jupyter, Colab)\n",
                "    # Vai um n√≠vel ACIMA do diret√≥rio atual (que deve ser 'notebooks/')\n",
                "    # para chegar √† raiz do projeto onde 'src/' se encontra.\n",
                "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..')) # <<< CORRE√á√ÉO AQUI ('..' em vez de '.')\n",
                "    \n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "    print(f\"Adicionado '{project_root}' ao sys.path para encontrar 'src'.\")\n",
                "else:\n",
                "    print(f\"'{project_root}' j√° est√° no sys.path.\")\n",
                "\n",
                "\n",
                "# --- Imports do Nosso Projeto (de 'src/') ---\n",
                "try:\n",
                "    from src.sdp_model import BiasAwareSDP\n",
                "    from src.heuristic import EnhancedLouvainWithBias\n",
                "    from src.evaluation import ComprehensiveEvaluator\n",
                "    from src.data_utils import generate_misaligned_network, generate_twibot_like_network\n",
                "    print(\"‚úÖ Classes e fun√ß√µes do projeto ('src/') importadas com sucesso!\")\n",
                "except (ImportError, ModuleNotFoundError) as e:\n",
                "    print(f\"‚ö†Ô∏è ERRO AO IMPORTAR DE 'SRC': {e}\")\n",
                "    print(\"   - Verifique se o notebook est√° na pasta 'notebooks/' e os arquivos .py est√£o em 'src/'.\")\n",
                "    print(\"   - Certifique-se de que a pasta raiz do projeto foi adicionada corretamente ao sys.path acima.\")\n",
                "    print(\"   - Certifique-se de que h√° um arquivo vazio '__init__.py' em 'src/'.\")\n",
                "    raise e\n",
                "\n",
                "# --- Configura√ß√µes Gerais ---\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette(\"husl\")\n",
                "np.random.seed(42)\n",
                "random.seed(42)\n",
                "\n",
                "# --- Configura√ß√£o do TwiBot-22 ---\n",
                "TWIBOT_PATH = os.path.join(project_root, \"data\", \"TwiBot22\") # Caminho relativo √† raiz do projeto\n",
                "if not os.path.exists(TWIBOT_PATH):\n",
                "    print(f\"‚ö†Ô∏è AVISO: Diret√≥rio TwiBot-22 n√£o encontrado em '{TWIBOT_PATH}'. Verifique o caminho.\")\n",
                "else:\n",
                "    print(f\"‚úÖ Usando dados do TwiBot-22 em: {TWIBOT_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Zv9D8wd9ougA"
            },
            "source": [
                "## 3. Exemplo: Karate Club"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 828
                },
                "id": "o5PADoduougA",
                "outputId": "fca1a8f0-0c7a-4c7a-af98-19460e21fe43"
            },
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EXEMPLO: KARATE CLUB\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "G_karate = nx.karate_club_graph()\n",
                "print(f\"\\nN√≥s: {G_karate.number_of_nodes()}, Arestas: {G_karate.number_of_edges()}\")\n",
                "\n",
                "# Simular vi√©s\n",
                "bias_karate = {}\n",
                "for node in G_karate.nodes():\n",
                "    base = 0.7 if node < 17 else -0.7\n",
                "    bias_karate[node] = np.clip(base + np.random.normal(0, 0.2), -1, 1)\n",
                "\n",
                "# Simular bots\n",
                "bot_nodes = random.sample(list(G_karate.nodes()), int(G_karate.number_of_nodes() * 0.1))\n",
                "bot_karate = {node: node in bot_nodes for node in G_karate.nodes()}\n",
                "\n",
                "# Louvain\n",
                "print(\"\\nüîç Louvain...\")\n",
                "partition_louvain = community_louvain.best_partition(G_karate)\n",
                "metrics_louvain = ComprehensiveEvaluator.evaluate_communities(G_karate, partition_louvain, bias_karate, bot_karate)\n",
                "print(f\"  Modularidade: {metrics_louvain['modularity']:.4f}\")\n",
                "print(f\"  Separa√ß√£o de vi√©s: {metrics_louvain['bias_separation']:.4f}\")\n",
                "\n",
                "# SDP\n",
                "print(\"\\nüîç SDP (Œ±=0.5)...\")\n",
                "detector_sdp = BiasAwareSDP(alpha=0.5, verbose=False)\n",
                "detector_sdp.fit(G_karate, bias_karate)\n",
                "partition_sdp = detector_sdp.get_communities()\n",
                "metrics_sdp = ComprehensiveEvaluator.evaluate_communities(G_karate, partition_sdp, bias_karate, bot_karate)\n",
                "print(f\"  Modularidade: {metrics_sdp['modularity']:.4f} ({(metrics_sdp['modularity']/metrics_louvain['modularity']-1)*100:+.1f}%)\")\n",
                "print(f\"  Separa√ß√£o de vi√©s: {metrics_sdp['bias_separation']:.4f} ({(metrics_sdp['bias_separation']/metrics_louvain['bias_separation']-1)*100:+.1f}%)\")\n",
                "print(f\"  Tempo: {detector_sdp.execution_time:.3f}s\")\n",
                "\n",
                "# Visualiza√ß√£o\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "pos = nx.spring_layout(G_karate, seed=42)\n",
                "\n",
                "nx.draw_networkx(G_karate, pos, node_color=[partition_louvain[n] for n in G_karate.nodes()],\n",
                "                 cmap='Set3', with_labels=True, node_size=500, ax=axes[0], font_size=8)\n",
                "axes[0].set_title(f'Louvain\\nMod: {metrics_louvain[\"modularity\"]:.3f}', fontweight='bold')\n",
                "axes[0].axis('off')\n",
                "\n",
                "nx.draw_networkx(G_karate, pos, node_color=[partition_sdp[n] for n in G_karate.nodes()],\n",
                "                 cmap='Set3', with_labels=True, node_size=500, ax=axes[1], font_size=8)\n",
                "axes[1].set_title(f'SDP (Œ±=0.5)\\nSep: {metrics_sdp[\"bias_separation\"]:.3f}', fontweight='bold')\n",
                "axes[1].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Exemplo conclu√≠do!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "WRXDzpFeougA"
            },
            "source": [
                "## 4. Compara√ß√£o SDP vs Heur√≠stica"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "iqOKHezwougA",
                "outputId": "14129ef9-fc46-4961-9b65-340fd26dd0e0"
            },
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*90)\n",
                "print(\"COMPARA√á√ÉO: SDP vs HEUR√çSTICA\")\n",
                "print(\"=\"*90)\n",
                "\n",
                "G, bias_scores, bot_labels = generate_misaligned_network(n_nodes=100)\n",
                "print(f\"\\nRede: {G.number_of_nodes()} n√≥s, {G.number_of_edges()} arestas\")\n",
                "\n",
                "results = []\n",
                "alphas = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
                "\n",
                "# Louvain baseline\n",
                "partition_louvain = community_louvain.best_partition(G)\n",
                "metrics_louvain = ComprehensiveEvaluator.evaluate_communities(G, partition_louvain, bias_scores, bot_labels)\n",
                "results.append({'method': 'Louvain', 'alpha': None, **metrics_louvain})\n",
                "\n",
                "print(\"\\nüîç Testando SDP...\")\n",
                "for alpha in alphas:\n",
                "    detector = BiasAwareSDP(alpha=alpha, verbose=False)\n",
                "    detector.fit(G, bias_scores)\n",
                "    partition = detector.get_communities()\n",
                "    metrics = ComprehensiveEvaluator.evaluate_communities(G, partition, bias_scores, bot_labels)\n",
                "    results.append({'method': 'SDP', 'alpha': alpha, 'time': detector.execution_time, **metrics})\n",
                "    print(f\"  Œ±={alpha}: Sep={metrics['bias_separation']:.3f}, Tempo={detector.execution_time:.3f}s\")\n",
                "\n",
                "print(\"\\nüîç Testando Heur√≠stica...\")\n",
                "for alpha in alphas:\n",
                "    detector = EnhancedLouvainWithBias(alpha=alpha, verbose=False)\n",
                "    detector.fit(G, bias_scores, num_communities=2)\n",
                "    partition = detector.get_communities()\n",
                "    metrics = ComprehensiveEvaluator.evaluate_communities(G, partition, bias_scores, bot_labels)\n",
                "    results.append({'method': 'Heur√≠stica', 'alpha': alpha, 'time': detector.execution_time, **metrics})\n",
                "    print(f\"  Œ±={alpha}: Sep={metrics['bias_separation']:.3f}, Tempo={detector.execution_time:.3f}s\")\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "\n",
                "print(\"\\nüìä Resultados:\")\n",
                "print(df[['method', 'alpha', 'modularity', 'bias_separation', 'time']].to_string(index=False))\n",
                "\n",
                "# Visualiza√ß√£o\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "df_sdp = df[df['method'] == 'SDP']\n",
                "df_heur = df[df['method'] == 'Heur√≠stica']\n",
                "\n",
                "axes[0].plot(df_sdp['alpha'], df_sdp['bias_separation'], 'o-', label='SDP', linewidth=2, markersize=8)\n",
                "axes[0].plot(df_heur['alpha'], df_heur['bias_separation'], 's--', label='Heur√≠stica', linewidth=2, markersize=8)\n",
                "axes[0].axhline(y=metrics_louvain['bias_separation'], color='red', linestyle=':', label='Louvain')\n",
                "axes[0].set_xlabel('Alpha (Œ±)')\n",
                "axes[0].set_ylabel('Separa√ß√£o de Vi√©s')\n",
                "axes[0].set_title('Qualidade: SDP vs Heur√≠stica')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(df_sdp['alpha'], df_sdp['time'], 'o-', label='SDP', linewidth=2, markersize=8)\n",
                "axes[1].plot(df_heur['alpha'], df_heur['time'], 's--', label='Heur√≠stica', linewidth=2, markersize=8)\n",
                "axes[1].set_xlabel('Alpha (Œ±)')\n",
                "axes[1].set_ylabel('Tempo (s)')\n",
                "axes[1].set_title('Efici√™ncia Computacional')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "axes[1].set_yscale('log')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Compara√ß√£o conclu√≠da!\")\n",
                "print(f\"\\nüí° Conclus√£o: Heur√≠stica √© ~{df_sdp['time'].mean()/df_heur['time'].mean():.0f}x mais r√°pida com resultados equivalentes!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2Fp-1RJGougA"
            },
            "source": [
                "## 5. TwiBot-22: Dataset Real\n",
                "\n",
                "Dataset Real (Requer Download)\n",
                "\n",
                "Para usar o TwiBot-22 real:\n",
                "1. Acesse: https://github.com/LuoUndergradXJTU/TwiBot-22\n",
                "2. Solicite acesso\n",
                "3. Baixe e mova para a pasta data, na subpasta TwiBot22\n",
                "\n",
                "Nesta se√ß√£o, aplicaremos a metodologia ao dataset TwiBot-22 real.\n",
                "Utilizaremos a **Heur√≠stica Eficiente (`EnhancedLouvainWithBias`)** devido √† escala do grafo."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Configura√ß√£o e imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "rXtq6PCRougB",
                "outputId": "6f637aec-63b1-49d1-e84e-9204f4f1255a"
            },
            "outputs": [],
            "source": [
                "# Imports espec√≠ficos para esta se√ß√£o (adicionar aos imports gerais se preferir)\n",
                "import pandas as pd\n",
                "import json\n",
                "import glob\n",
                "import os # Para verificar a exist√™ncia do diret√≥rio\n",
                "\n",
                "# Imports das nossas classes (j√° devem estar na C√©lula 3 do notebook atualizado)\n",
                "# from sdp_model import BiasAwareSDP # (N√£o usaremos aqui devido √† escala)\n",
                "# from heuristic import EnhancedLouvainWithBias\n",
                "# from evaluation import ComprehensiveEvaluator\n",
                "# from data_utils import generate_twibot_like_network # (N√£o usaremos aqui, mas pode manter o import)\n",
                "\n",
                "# --- Configura√ß√£o ---\n",
                "# AJUSTE ESTE CAMINHO para onde voc√™ descompactou o TwiBot-22\n",
                "TWIBOT_PATH = \"../data/TwiBot22\" \n",
                "\n",
                "# Verificar se o diret√≥rio existe\n",
                "if not os.path.exists(TWIBOT_PATH):\n",
                "    print(f\"‚ö†Ô∏è ERRO: Diret√≥rio TwiBot-22 n√£o encontrado em '{TWIBOT_PATH}'.\")\n",
                "    print(\"   Por favor, ajuste a vari√°vel TWIBOT_PATH ou fa√ßa upload dos dados.\")\n",
                "    # Voc√™ pode querer parar a execu√ß√£o aqui ou usar dados simulados como fallback\n",
                "    # raise FileNotFoundError(f\"Diret√≥rio TwiBot-22 n√£o encontrado em {TWIBOT_PATH}\")\n",
                "else:\n",
                "    print(f\"‚úÖ Usando dados do TwiBot-22 em: {TWIBOT_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Carregando Labels e Arestas\n",
                "\n",
                "Carregamos os r√≥tulos de bot/humano (`label.csv`) e as conex√µes do grafo (`edge.csv`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 1: CONSTRU√á√ÉO OTIMIZADA DO GRAFO (IGRAPH - COM SAVE/LOAD ATIVADO)\n",
                "\n",
                "import pandas as pd\n",
                "import json\n",
                "import igraph as ig\n",
                "import os\n",
                "import time\n",
                "import gc\n",
                "import pickle\n",
                "\n",
                "# --- Nomes dos arquivos para salvar/carregar ---\n",
                "graph_save_file = \"igraph_real_graph.pkl\"\n",
                "labels_save_file = \"igraph_real_labels.json\"\n",
                "id_map_save_file = \"igraph_id_maps.json\"\n",
                "\n",
                "# --- Tentar carregar os arquivos pr√©-processados ---\n",
                "print(f\"üíæ Verificando se arquivos de grafo pr√©-processado existem...\")\n",
                "graph_loaded_successfully = False # Assumir que n√£o carregou inicialmente\n",
                "if os.path.exists(graph_save_file) and os.path.exists(labels_save_file) and os.path.exists(id_map_save_file):\n",
                "    print(f\"   Arquivos encontrados! Carregando grafo, labels e mapas pr√©-processados...\")\n",
                "    try:\n",
                "        # Carregar grafo igraph\n",
                "        with open(graph_save_file, 'rb') as f:\n",
                "            G_igraph_real = pickle.load(f)\n",
                "        \n",
                "        # Carregar dicion√°rio de labels (chaves s√£o str no JSON, converter para int)\n",
                "        with open(labels_save_file, 'r', encoding='utf-8') as f:\n",
                "            bot_labels_sub_igraph_str_keys = json.load(f)\n",
                "            bot_labels_sub_igraph = {int(k): v for k, v in bot_labels_sub_igraph_str_keys.items()}\n",
                "\n",
                "        # Carregar mapeamentos de ID (chaves do rev_map s√£o str no JSON, converter para int)\n",
                "        with open(id_map_save_file, 'r', encoding='utf-8') as f:\n",
                "            id_maps = json.load(f)\n",
                "            user_id_map = id_maps['user_id_map'] # str -> int\n",
                "            user_id_rev_map = {int(k): v for k, v in id_maps['user_id_rev_map'].items()} # int -> str\n",
                "\n",
                "        # Verificar se o grafo carregado parece v√°lido\n",
                "        if isinstance(G_igraph_real, ig.Graph) and G_igraph_real.vcount() > 1:\n",
                "            print(f\"   ‚úÖ Grafo igraph carregado: {G_igraph_real.vcount():,} n√≥s, {G_igraph_real.ecount():,} arestas.\")\n",
                "            print(f\"   ‚úÖ Labels carregados para {len(bot_labels_sub_igraph):,} n√≥s.\")\n",
                "            print(f\"   ‚úÖ Mapeamentos de ID carregados.\")\n",
                "            graph_loaded_successfully = True # Marcar como carregado com sucesso\n",
                "        else:\n",
                "            print(f\"   ‚ö†Ô∏è Arquivo de grafo '{graph_save_file}' inv√°lido ou vazio. Recalculando...\")\n",
                "            # Limpar vari√°veis se o grafo carregado for inv√°lido\n",
                "            del G_igraph_real \n",
                "            del bot_labels_sub_igraph \n",
                "            del user_id_map\n",
                "            del user_id_rev_map\n",
                "            # gc.collect()\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ö†Ô∏è Erro ao carregar arquivos: {e}. Recalculando grafo...\")\n",
                "        # Limpar vari√°veis em caso de erro\n",
                "        if 'G_igraph_real' in locals(): del G_igraph_real\n",
                "        if 'bot_labels_sub_igraph' in locals(): del bot_labels_sub_igraph\n",
                "        if 'user_id_map' in locals(): del user_id_map\n",
                "        if 'user_id_rev_map' in locals(): del user_id_rev_map\n",
                "        # gc.collect()\n",
                "else:\n",
                "    print(f\"   Arquivos n√£o encontrados. Grafo ser√° constru√≠do.\")\n",
                "\n",
                "# --- Executar constru√ß√£o completa APENAS se n√£o carregou os arquivos ---\n",
                "if not graph_loaded_successfully:\n",
                "    print(\"\\n--- Iniciando constru√ß√£o completa do grafo ---\")\n",
                "    \n",
                "    # --- Carregar Labels (Original) ---\n",
                "    try:\n",
                "        label_df = pd.read_csv(f\"{TWIBOT_PATH}/label.csv\")\n",
                "        bot_labels_real = dict(zip(label_df['id'].astype(str), label_df['label'] == 'bot'))\n",
                "        user_ids_str_list = sorted(list(bot_labels_real.keys()))\n",
                "        user_id_map = {user_id: i for i, user_id in enumerate(user_ids_str_list)}\n",
                "        user_id_rev_map = {i: user_id for user_id, i in user_id_map.items()}\n",
                "        valid_user_ids_str_set = set(bot_labels_real.keys())\n",
                "        print(f\"üìä Carregados {len(bot_labels_real):,} r√≥tulos.\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è ERRO ao carregar label.csv: {e}\")\n",
                "        raise e\n",
                "\n",
                "    # --- Construir Grafo igraph lendo edge.csv em Chunks (Original) ---\n",
                "    print(\"\\n‚öôÔ∏è Processando edge.csv em chunks...\")\n",
                "    start_time_graph = time.time()\n",
                "    chunk_size = 100000 # Manter chunk pequeno\n",
                "    print(f\"   Usando chunk size: {chunk_size:,}\")\n",
                "    edge_file_path = f\"{TWIBOT_PATH}/edge.csv\"\n",
                "    user_relations = ['following', 'followers']\n",
                "    G_igraph_full = ig.Graph(n=len(user_ids_str_list), directed=False)\n",
                "    G_igraph_full.vs[\"name\"] = user_ids_str_list\n",
                "    added_edges_count = 0\n",
                "    \n",
                "    try:\n",
                "        edge_iterator = pd.read_csv(edge_file_path, chunksize=chunk_size, iterator=True, low_memory=True)\n",
                "        for i, chunk in enumerate(edge_iterator):\n",
                "            if i % 10 == 0: print(f\"   Processando chunk {i+1}...\")\n",
                "            # ... (c√≥digo interno do loop como antes, filtrando e adicionando arestas) ...\n",
                "            chunk['source_id_str'] = chunk['source_id'].astype(str)\n",
                "            chunk['target_id_str'] = chunk['target_id'].astype(str)\n",
                "            filtered_chunk = chunk[\n",
                "                chunk['relation'].isin(user_relations) &\n",
                "                chunk['source_id_str'].isin(valid_user_ids_str_set) &\n",
                "                chunk['target_id_str'].isin(valid_user_ids_str_set)\n",
                "            ].copy()\n",
                "            filtered_chunk['source_int'] = filtered_chunk['source_id_str'].map(user_id_map)\n",
                "            filtered_chunk['target_int'] = filtered_chunk['target_id_str'].map(user_id_map)\n",
                "            filtered_chunk.dropna(subset=['source_int', 'target_int'], inplace=True)\n",
                "            filtered_chunk['source_int'] = filtered_chunk['source_int'].astype(int)\n",
                "            filtered_chunk['target_int'] = filtered_chunk['target_int'].astype(int)\n",
                "            edges_to_add = list(zip(filtered_chunk['source_int'], filtered_chunk['target_int']))\n",
                "            if edges_to_add:\n",
                "                G_igraph_full.add_edges(edges_to_add)\n",
                "                added_edges_count += len(edges_to_add)\n",
                "            del chunk, filtered_chunk, edges_to_add\n",
                "\n",
                "        end_time_graph = time.time()\n",
                "        print(f\"\\n‚úÖ Grafo igraph inicial constru√≠do em {end_time_graph - start_time_graph:.2f} segundos.\")\n",
                "        print(f\"   ‚Ü≥ {G_igraph_full.vcount():,} n√≥s, {G_igraph_full.ecount():,} arestas ({added_edges_count:,} arestas adicionadas).\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è ERRO inesperado ao processar edge.csv: {e}\")\n",
                "        raise\n",
                "\n",
                "    # --- Limpeza ---\n",
                "    del label_df\n",
                "    del valid_user_ids_str_set\n",
                "    print(\"\\nüßπ Mem√≥ria dos DataFrames liberada.\")\n",
                "    gc.collect()\n",
                "\n",
                "    # --- Obter Maior Componente Conectado ---\n",
                "    G_igraph_real = None # Inicializar\n",
                "    bot_labels_sub_igraph = {}\n",
                "    \n",
                "    if G_igraph_full.vcount() > 0:\n",
                "        print(\"\\n‚öôÔ∏è Encontrando o maior componente conectado (igraph)...\")\n",
                "        try:\n",
                "            components = G_igraph_full.components(mode=ig.WEAK)\n",
                "            if not components: # Verifica se components est√° vazio\n",
                "                 print(\"   ‚ö†Ô∏è Nenhum componente encontrado.\")\n",
                "                 G_igraph_real = ig.Graph() # Define grafo vazio\n",
                "            else:\n",
                "                giant_component = components.giant() # Pega o componente gigante\n",
                "                largest_cc_indices = giant_component.vs.indices # Pega os √≠ndices\n",
                "                print(f\"   ‚Ü≥ Encontrado componente gigante com {len(largest_cc_indices):,} n√≥s.\")\n",
                "                \n",
                "                G_igraph_real = G_igraph_full.subgraph(largest_cc_indices)\n",
                "                \n",
                "                subgraph_node_names = G_igraph_real.vs[\"name\"]\n",
                "                bot_labels_sub_igraph = {user_id_map[name]: bot_labels_real.get(name, False)\n",
                "                                         for name in subgraph_node_names}\n",
                "\n",
                "                print(f\"üìä Usando maior componente conectado (igraph): {G_igraph_real.vcount():,} n√≥s, {G_igraph_real.ecount():,} arestas.\")\n",
                "                num_bots_in_subgraph = sum(bot_labels_sub_igraph.values())\n",
                "                print(f\"   ‚Ü≥ Cont√©m {num_bots_in_subgraph:,} bots ({num_bots_in_subgraph / G_igraph_real.vcount():.1%})\")\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è ERRO ao encontrar/criar subgrafo: {e}\")\n",
                "            G_igraph_real = ig.Graph()\n",
                "\n",
                "        print(\"   Liberando mem√≥ria do grafo completo...\")\n",
                "        del G_igraph_full\n",
                "        gc.collect()\n",
                "\n",
                "        # --- SALVAR OS RESULTADOS ---\n",
                "        if G_igraph_real is not None and G_igraph_real.vcount() > 1:\n",
                "            print(f\"\\nüíæ Salvando grafo processado em '{graph_save_file}'...\")\n",
                "            try:\n",
                "                with open(graph_save_file, 'wb') as f:\n",
                "                    pickle.dump(G_igraph_real, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
                "                print(f\"   ‚úÖ Grafo salvo.\")\n",
                "            except Exception as e: print(f\"   ‚ö†Ô∏è Erro ao salvar grafo: {e}\")\n",
                "\n",
                "            print(f\"\\nüíæ Salvando labels do subgrafo em '{labels_save_file}'...\")\n",
                "            try:\n",
                "                labels_to_save = {str(k): v for k, v in bot_labels_sub_igraph.items()}\n",
                "                with open(labels_save_file, 'w', encoding='utf-8') as f: json.dump(labels_to_save, f)\n",
                "                print(f\"   ‚úÖ Labels salvos.\")\n",
                "            except Exception as e: print(f\"   ‚ö†Ô∏è Erro ao salvar labels: {e}\")\n",
                "\n",
                "            print(f\"\\nüíæ Salvando mapeamentos de ID em '{id_map_save_file}'...\")\n",
                "            try:\n",
                "                rev_map_to_save = {str(k): v for k, v in user_id_rev_map.items()}\n",
                "                id_maps_to_save = {'user_id_map': user_id_map, 'user_id_rev_map': rev_map_to_save}\n",
                "                with open(id_map_save_file, 'w', encoding='utf-8') as f: json.dump(id_maps_to_save, f)\n",
                "                print(f\"   ‚úÖ Mapeamentos salvos.\")\n",
                "            except Exception as e: print(f\"   ‚ö†Ô∏è Erro ao salvar mapas: {e}\")\n",
                "        else:\n",
                "             print(\"\\n‚ö†Ô∏è Grafo final inv√°lido ou muito pequeno. Arquivos N√ÉO foram salvos.\")\n",
                "\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è Grafo inicial vazio. Nada foi salvo.\")\n",
                "        G_igraph_real = ig.Graph()\n",
                "        bot_labels_sub_igraph = {}\n",
                "        user_id_map = {}\n",
                "        user_id_rev_map = {}\n",
                "\n",
                "# --- Fim do Bloco if not graph_loaded_successfully ---\n",
                "\n",
                "# --- VERIFICA√á√ÉO FINAL ---\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"VERIFICA√á√ÉO FINAL NO FIM DA C√âLULA 1:\")\n",
                "# ... (bloco de verifica√ß√£o final como antes) ...\n",
                "if 'G_igraph_real' in locals() and G_igraph_real is not None:\n",
                "    print(f\"  Tipo de G_igraph_real: {type(G_igraph_real)}\")\n",
                "    print(f\"  N√∫mero FINAL de n√≥s: {G_igraph_real.vcount():,}\")\n",
                "    # ... (restante da verifica√ß√£o)\n",
                "else:\n",
                "    print(\"  ERRO: G_igraph_real N√ÉO est√° definido ou √© None!\")\n",
                "print(\"=\"*50 + \"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.4 C√°lculo dos Scores de Vi√©s (Placeholder)\n",
                "\n",
                "Esta √© a etapa mais cr√≠tica. O c√≥digo abaixo l√™ os arquivos de tweets e extrai os textos. **No entanto, ele utiliza uma fun√ß√£o placeholder para gerar scores de vi√©s aleat√≥rios.**\n",
                "\n",
                "**Para resultados reais, voc√™ deve:**\n",
                "1.  Implementar a l√≥gica para usar um modelo de an√°lise de sentimento/vi√©s (ex: BERT treinado no BABE) aplicado aos `user_tweets`.\n",
                "2.  Substituir a linha `bias_scores_real[user_id] = np.tanh(...)` pela chamada ao seu modelo.\n",
                "3.  Tratar usu√°rios sem tweets (atribuindo vi√©s neutro 0.0, por exemplo)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# C√âLULA 2: C√ÅLCULO DE VI√âS COM LLM (DUAS PASSAGENS + ORDENA√á√ÉO)\n",
                "\n",
                "# --- Imports Necess√°rios ---\n",
                "from collections import defaultdict\n",
                "import numpy as np\n",
                "import json\n",
                "import glob\n",
                "import gc\n",
                "import csv\n",
                "import os\n",
                "import psutil # Para monitorar mem√≥ria (requer: pip install psutil)\n",
                "import pandas as pd # Para a tentativa de ordena√ß√£o\n",
                "import multiprocessing as mp # Para paralelismo\n",
                "import time\n",
                "# Imports para o LLM (exemplo com Hugging Face Transformers)\n",
                "# Certifique-se de instalar: pip install transformers torch ou transformers tensorflow\n",
                "from transformers import pipeline # Exemplo simples\n",
                "import torch # Se usar PyTorch\n",
                "\n",
                "# Assumindo igraph (ig), G_igraph_real, id_map_save_file, TWIBOT_PATH definidos na C√âLULA 1\n",
                "\n",
                "# --- Verifica√ß√£o Inicial do Grafo ---\n",
                "print(\"-\" * 50)\n",
                "print(\"VERIFICANDO O GRAFO ANTES DE INICIAR A C√âLULA 2:\")\n",
                "try:\n",
                "    print(f\"  Tipo de G_igraph_real: {type(G_igraph_real)}\")\n",
                "    print(f\"  N√∫mero de n√≥s em G_igraph_real: {G_igraph_real.vcount():,}\")\n",
                "    print(f\"  N√∫mero de arestas em G_igraph_real: {G_igraph_real.ecount():,}\")\n",
                "    print(f\"  Primeiros 5 nomes de n√≥s: {G_igraph_real.vs['name'][:5]}\")\n",
                "except NameError:\n",
                "    print(\"  ERRO: G_igraph_real N√ÉO EST√Å DEFINIDO! Execute a C√©lula 1.\")\n",
                "    raise\n",
                "except Exception as e:\n",
                "    print(f\"  ERRO ao acessar G_igraph_real: {e}\")\n",
                "    raise\n",
                "print(\"-\" * 50)\n",
                "\n",
                "# --- Fun√ß√£o Auxiliar para Monitorar Mem√≥ria ---\n",
                "def print_memory_usage(label=\"\"):\n",
                "    \"\"\"Imprime o uso atual de mem√≥ria RAM do processo.\"\"\"\n",
                "    try:\n",
                "        process = psutil.Process(os.getpid())\n",
                "        mem_info = process.memory_info()\n",
                "        print(f\"   {label} RAM Usada: {mem_info.rss / (1024 * 1024):,.1f} MB\")\n",
                "    except Exception as e_mem: print(f\"   {label} Aviso: N√£o foi poss√≠vel obter RAM: {e_mem}\")\n",
                "\n",
                "# --- Nomes dos Arquivos ---\n",
                "intermediate_text_file_base = \"intermediate_texts_part\"\n",
                "intermediate_text_file_combined = \"intermediate_texts_combined.tsv\"\n",
                "sorted_intermediate_text_file = \"intermediate_texts_sorted.tsv\"\n",
                "bias_scores_file = \"calculated_bias_scores.json\"\n",
                "\n",
                "# --- Verificar se o Resultado Final J√° Existe ---\n",
                "print(f\"üíæ Verificando se o arquivo final '{bias_scores_file}' j√° existe...\")\n",
                "calculation_needed = True\n",
                "bias_scores_real = None\n",
                "if os.path.exists(bias_scores_file):\n",
                "    print(f\"   Arquivo final encontrado! Tentando carregar...\")\n",
                "    try:\n",
                "        with open(bias_scores_file, 'r', encoding='utf-8') as f: bias_scores_real = json.load(f)\n",
                "        if isinstance(bias_scores_real, dict) and bias_scores_real:\n",
                "            print(f\"   ‚úÖ Scores carregados para {len(bias_scores_real)} usu√°rios.\")\n",
                "            try: # Verificar n√≥s do grafo atual\n",
                "                nodes_in_graph_set = set(G_igraph_real.vs[\"name\"])\n",
                "                missing = [n for n in nodes_in_graph_set if n not in bias_scores_real]\n",
                "                if missing: print(f\"   ‚ö†Ô∏è {len(missing)} n√≥s do grafo atual sem score. Atribuindo 0.0.\");\n",
                "                for node in missing: bias_scores_real[node] = 0.0\n",
                "                calculation_needed = False\n",
                "            except NameError: calculation_needed = False # Confiar no carregamento\n",
                "            except Exception: calculation_needed = True; bias_scores_real = None\n",
                "        else: calculation_needed = True; bias_scores_real = None\n",
                "    except Exception as e: calculation_needed = True; bias_scores_real = None\n",
                "else: print(f\"   Arquivo final n√£o encontrado. Calculando...\")\n",
                "\n",
                "# --- Executar C√°lculo Apenas se Necess√°rio ---\n",
                "if calculation_needed:\n",
                "    print(\"\\n--- Iniciando c√°lculo completo de scores de vi√©s com LLM (Duas Passagens) ---\")\n",
                "\n",
                "    # --- Carregar Mapeamentos de ID e N√≥s V√°lidos ---\n",
                "    print(\"\\n‚öôÔ∏è Carregando mapeamentos de ID e n√≥s do grafo...\")\n",
                "    try:\n",
                "        with open(id_map_save_file, 'r', encoding='utf-8') as f: id_maps = json.load(f)\n",
                "        user_id_map = id_maps['user_id_map']; user_id_rev_map = {int(k): v for k, v in id_maps['user_id_rev_map'].items()}\n",
                "        graph_nodes_set = set(G_igraph_real.vs[\"name\"])\n",
                "        print(f\"   ‚úÖ Mapas/N√≥s carregados ({len(graph_nodes_set):,} n√≥s v√°lidos).\")\n",
                "        print_memory_usage(\"Ap√≥s carregar IDs/N√≥s:\")\n",
                "    except Exception as e: print(f\"‚ö†Ô∏è ERRO carregando dados iniciais: {e}\"); raise\n",
                "\n",
                "    # --- Passagem 1: Filtrar Tweets PARALELAMENTE e Salvar user_id_int, tweet_text ---\n",
                "    def process_and_save_texts(args_tuple):\n",
                "        \"\"\"L√™ arquivos, filtra por usu√°rio e salva ID (int) e Texto.\"\"\"\n",
                "        list_of_tweet_files, worker_num, nodes_valid_set, user_str_to_int_map, output_file_base = args_tuple\n",
                "        output_filename = f\"{output_file_base}_{worker_num}.tsv\"\n",
                "        count_saved = 0; count_lines = 0\n",
                "        try:\n",
                "            # print(f\"   [Worker {worker_num}] Iniciando...\") # Verbose\n",
                "            with open(output_filename, 'w', newline='', encoding='utf-8') as outfile:\n",
                "                writer = csv.writer(outfile, delimiter='\\t')\n",
                "                for tweet_file_path in list_of_tweet_files:\n",
                "                    try:\n",
                "                        with open(tweet_file_path, 'r', encoding='utf-8') as infile:\n",
                "                            for line in infile:\n",
                "                                count_lines += 1\n",
                "                                try:\n",
                "                                    tweet_data = json.loads(line)\n",
                "                                    user_id_str = tweet_data.get('author_id')\n",
                "                                    if user_id_str and user_id_str in nodes_valid_set:\n",
                "                                        tweet_text = tweet_data.get('text', '').replace('\\n', ' ').replace('\\t', ' ')\n",
                "                                        if tweet_text:\n",
                "                                            user_id_int = user_str_to_int_map.get(user_id_str)\n",
                "                                            if user_id_int is not None:\n",
                "                                                writer.writerow([user_id_int, tweet_text])\n",
                "                                                count_saved += 1\n",
                "                                except (json.JSONDecodeError, AttributeError): continue\n",
                "                                finally: del tweet_data\n",
                "                    except Exception as e_file: print(f\"   [Worker {worker_num}] Erro {os.path.basename(tweet_file_path)}: {e_file}\")\n",
                "            print(f\"   [Worker {worker_num}] Conclu√≠do ({count_saved:,} textos salvos de {count_lines:,} linhas)\")\n",
                "            return output_filename, count_saved\n",
                "        except Exception as e_worker: print(f\"   [Worker {worker_num}] ERRO FATAL: {e_worker}\"); return None, 0\n",
                "\n",
                "    tweet_files = sorted(glob.glob(f\"{TWIBOT_PATH}/tweet_*.json\"))\n",
                "    partial_files_info = []\n",
                "    processed_tweets_pass1 = 0\n",
                "    if not tweet_files: print(f\"‚ö†Ô∏è AVISO: Nenhum arquivo tweet_*.json encontrado.\")\n",
                "    else:\n",
                "        num_workers = max(1, mp.cpu_count() - 1)\n",
                "        print(f\"\\n--- Passagem 1: Extraindo textos ({num_workers} workers) ---\")\n",
                "        start_pass1 = time.time()\n",
                "        files_per_worker = [[] for _ in range(num_workers)]\n",
                "        for i, f in enumerate(tweet_files): files_per_worker[i % num_workers].append(f)\n",
                "        pool_args = [(files_per_worker[i], i, graph_nodes_set, user_id_map, intermediate_text_file_base) for i in range(num_workers) if files_per_worker[i]]\n",
                "        print(\"   Limpando arquivos parciais antigos...\");\n",
                "        for f_old in glob.glob(f\"{intermediate_text_file_base}_*.tsv\"):\n",
                "             # --- CORRE√á√ÉO ESTAVA AQUI ---\n",
                "            try:\n",
                "                if os.path.exists(f_old): # Verificar antes de remover\n",
                "                     os.remove(f_old)\n",
                "            except Exception as e_remove:\n",
                "                print(f\"      Aviso: N√£o foi poss√≠vel remover {f_old}: {e_remove}\")\n",
                "             # --- FIM DA CORRE√á√ÉO ---\n",
                "        try:\n",
                "            with mp.Pool(processes=len(pool_args)) as pool: results = pool.map(process_and_save_texts, pool_args)\n",
                "            for filename, count in results:\n",
                "                if filename: partial_files_info.append(filename); processed_tweets_pass1 += count\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è ERRO GERAL Passagem 1: {e}\")\n",
                "            # Limpar novamente em caso de erro durante o pool\n",
                "            for f_part in glob.glob(f\"{intermediate_text_file_base}_*.tsv\"):\n",
                "                 if os.path.exists(f_part): \n",
                "                    try: os.remove(f_part); \n",
                "                    except: pass\n",
                "            raise\n",
                "        end_pass1 = time.time()\n",
                "        if not partial_files_info: print(f\"\\n‚ö†Ô∏è Nenhum arquivo parcial gerado.\")\n",
                "        else: print(f\"\\nüìä Passagem 1 conclu√≠da em {end_pass1 - start_pass1:.2f}s ({processed_tweets_pass1:,} textos).\")\n",
                "\n",
                "    # --- Limpeza P√≥s-Passagem 1 ---\n",
                "    if 'graph_nodes_set' in locals(): del graph_nodes_set; gc.collect();\n",
                "    print_memory_usage(\"Ap√≥s Passagem 1:\")\n",
                "\n",
                "    # --- Concatenar Arquivos Parciais ---\n",
                "    intermediate_file_to_sort = None\n",
                "    if not partial_files_info: print(\"\\n‚ö†Ô∏è Pulando concatena√ß√£o/ordena√ß√£o.\")\n",
                "    else:\n",
                "        print(f\"\\n--- Concatenando {len(partial_files_info)} arquivos -> '{intermediate_text_file_combined}' ---\")\n",
                "        start_concat = time.time()\n",
                "        try:\n",
                "            if os.path.exists(intermediate_text_file_combined): os.remove(intermediate_text_file_combined)\n",
                "            with open(intermediate_file_combined, 'wb') as outfile:\n",
                "                 outfile.write(\"user_id_int\\ttweet_text\\n\".encode('utf-8')) # Cabe√ßalho\n",
                "                 for fname in partial_files_info:\n",
                "                     try:\n",
                "                         with open(fname, 'rb') as infile: outfile.write(infile.read())\n",
                "                         os.remove(fname)\n",
                "                     except Exception as e_concat_file: print(f\"      Erro ao concatenar {fname}: {e_concat_file}\")\n",
                "            end_concat = time.time(); print(f\"\\nüìä Concatena√ß√£o conclu√≠da em {end_concat - start_concat:.2f}s.\")\n",
                "            intermediate_file_to_sort = intermediate_file_combined\n",
                "        except Exception as e: print(f\"‚ö†Ô∏è ERRO concatena√ß√£o: {e}\"); raise\n",
                "\n",
                "    # --- Passagem Intermedi√°ria: Ordenar o Arquivo Concatenado ---\n",
                "    sort_method = \"N/A\"\n",
                "    if intermediate_file_to_sort and os.path.exists(intermediate_file_to_sort):\n",
                "        print(f\"\\n--- Ordenando '{intermediate_file_to_sort}' -> '{sorted_intermediate_file}' ---\")\n",
                "        start_sort = time.time()\n",
                "        try: # Tentar com Pandas\n",
                "            if os.path.exists(sorted_intermediate_file): os.remove(sorted_intermediate_file)\n",
                "            print(\"   Tentando ordenar com Pandas...\"); sort_chunk_size = 2000000\n",
                "            reader = pd.read_csv(intermediate_file_to_sort, delimiter='\\t', chunksize=sort_chunk_size, dtype={0: np.int64, 1: str}, low_memory=False, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
                "            all_chunks = []; print(f\"   Lendo chunks...\");\n",
                "            for i, chunk in enumerate(reader): print(f\"      Chunk {i+1}\"); all_chunks.append(chunk); gc.collect()\n",
                "            if not all_chunks: print(\"   ‚ö†Ô∏è Arquivo vazio.\"); sort_method = \"Pulado (vazio)\"\n",
                "            else:\n",
                "                print(\"   Concatenando/Ordenando...\"); full_df_temp = pd.concat(all_chunks, ignore_index=True); del all_chunks; gc.collect()\n",
                "                full_df_temp.sort_values(by='user_id_int', inplace=True, kind='mergesort')\n",
                "                print(f\"   Escrevendo '{sorted_intermediate_file}'...\"); full_df_temp.to_csv(sorted_intermediate_file, sep='\\t', index=False, header=True, chunksize=1000000, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
                "                del full_df_temp; gc.collect(); sort_method = \"Pandas\"\n",
                "        except MemoryError as me: # Fallback para ordena√ß√£o externa\n",
                "            print(f\"\\n   ‚ö†Ô∏è ERRO DE MEM√ìRIA com Pandas. Usando fallback: ordena√ß√£o externa.\");\n",
                "            sort_command = f\"(head -n 1 {intermediate_file_to_sort} && tail -n +2 {intermediate_file_to_sort} | sort -t$'\\\\t' -k1,1n -T .) > {sorted_intermediate_file}\"\n",
                "            print(f\"      Comando: {sort_command}\"); print(\"\\n      >>> PAUSADO. Execute o comando acima no terminal <<<\"); input(\"      >>> Aperte Enter AP√ìS terminar. <<<\")\n",
                "            if not os.path.exists(sorted_intermediate_file) or os.path.getsize(sorted_intermediate_file) < 10: raise RuntimeError(\"Arquivo ordenado externo falhou.\")\n",
                "            sort_method = \"Externo (OS sort)\"\n",
                "        except Exception as e: print(f\"   ‚ö†Ô∏è ERRO na ordena√ß√£o: {e}\"); raise\n",
                "        end_sort = time.time(); print(f\"\\nüìä Ordena√ß√£o ({sort_method}) conclu√≠da em {end_sort - start_sort:.2f}s.\")\n",
                "        # if os.path.exists(intermediate_file_to_sort): os.remove(intermediate_file_to_sort) # Opcional\n",
                "\n",
                "        # --- Passagem 2: Ler Arquivo Ordenado, Calcular Score com LLM (Batch) e Agregar ---\n",
                "        if sort_method != \"Pulado (vazio)\":\n",
                "            bias_scores_real = {} ; current_user_id_int = -1; current_score_sum = 0.0; current_tweet_count = 0\n",
                "            print(f\"\\n--- Passagem 2: Lendo '{sorted_intermediate_file}', usando LLM em batches ---\")\n",
                "            start_pass2 = time.time(); processed_lines_pass2 = 0\n",
                "            \n",
                "            # --- CARREGAR MODELO LLM E TOKENIZER ---\n",
                "            print(\"   Carregando modelo LLM (pode levar tempo)...\")\n",
                "            try:\n",
                "                device = 0 if torch.cuda.is_available() else -1\n",
                "                # *** SUBSTITUA PELO SEU MODELO DE VI√âS REAL ***\n",
                "                bias_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', tokenizer='distilbert-base-uncased-finetuned-sst-2-english', device=device)\n",
                "                print(f\"   ‚úÖ Modelo LLM carregado (dispositivo: {'GPU' if device == 0 else 'CPU'}).\")\n",
                "            except Exception as e_model:\n",
                "                print(f\"   ‚ö†Ô∏è Erro ao carregar modelo LLM: {e_model}. Usando placeholder.\")\n",
                "                bias_pipeline = None # Fallback\n",
                "\n",
                "            # --- Processar em Batches ---\n",
                "            batch_size = 32 # Ajuste conforme VRAM da GPU ou RAM da CPU\n",
                "            current_batch_texts = []\n",
                "            \n",
                "            try:\n",
                "                buffer_size = 10 * 1024 * 1024\n",
                "                with open(sorted_intermediate_file, 'r', newline='', encoding='utf-8', buffering=buffer_size) as infile:\n",
                "                    reader = csv.reader(infile, delimiter='\\t', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
                "                    header = next(reader)\n",
                "                    \n",
                "                    for row_num, row in enumerate(reader):\n",
                "                        processed_lines_pass2 += 1\n",
                "                        if len(row) == 2:\n",
                "                            try:\n",
                "                                user_id_int_cr = int(row[0]); tweet_text = row[1]\n",
                "                                \n",
                "                                # Se mudou o usu√°rio OU o batch est√° cheio\n",
                "                                if (user_id_int_cr != current_user_id_int and current_batch_texts) or len(current_batch_texts) >= batch_size:\n",
                "                                    if current_batch_texts: # Processar batch se n√£o estiver vazio\n",
                "                                        if bias_pipeline:\n",
                "                                            try: # Adicionar try/except em volta da infer√™ncia\n",
                "                                                results = bias_pipeline(current_batch_texts, truncation=True, max_length=512, batch_size=batch_size)\n",
                "                                                # Adapte o mapeamento de score conforme o output do SEU modelo\n",
                "                                                batch_scores = [res['score'] if res['label'] == 'POSITIVE' else -res['score'] if res['label'] == 'NEGATIVE' else 0.0 for res in results]\n",
                "                                            except Exception as e_infer:\n",
                "                                                 print(f\"\\n   ‚ö†Ô∏è Erro na infer√™ncia do LLM (batch a partir da linha ~{row_num}): {e_infer}. Usando placeholder para o batch.\")\n",
                "                                                 batch_scores = [0.0] * len(current_batch_texts) # Usar score neutro no erro\n",
                "                                        else: # Placeholder\n",
                "                                            batch_scores = [np.tanh((hash(txt) % 1000 - 500) / 250) for txt in current_batch_texts]\n",
                "                                        current_score_sum += sum(batch_scores)\n",
                "                                    current_batch_texts = [] # Limpar batch\n",
                "\n",
                "                                # Finalizar usu√°rio anterior se mudou\n",
                "                                if user_id_int_cr != current_user_id_int:\n",
                "                                    if current_user_id_int != -1 and current_tweet_count > 0:\n",
                "                                        avg = current_score_sum / current_tweet_count\n",
                "                                        user_str = user_id_rev_map.get(current_user_id_int)\n",
                "                                        if user_str: bias_scores_real[user_str] = avg\n",
                "                                    current_user_id_int = user_id_int_cr; current_score_sum = 0.0; current_tweet_count = 0\n",
                "\n",
                "                                # Adicionar tweet atual ao pr√≥ximo batch\n",
                "                                current_batch_texts.append(tweet_text)\n",
                "                                current_tweet_count += 1 # Contar tweets por usu√°rio\n",
                "\n",
                "                                # Feedback\n",
                "                                if (row_num + 1) % 100000 == 0:\n",
                "                                    print(f\"      ... linha {row_num+1:,}\", end=''); print_memory_usage()\n",
                "\n",
                "                            except (ValueError, KeyError) as ve: continue\n",
                "                \n",
                "                    # --- Processar √∫ltimo batch e √∫ltimo usu√°rio ---\n",
                "                    if current_batch_texts: # Processar √∫ltimo batch\n",
                "                         if bias_pipeline:\n",
                "                             try:\n",
                "                                results = bias_pipeline(current_batch_texts, truncation=True, max_length=512, batch_size=batch_size)\n",
                "                                batch_scores = [res['score'] if res['label'] == 'POSITIVE' else -res['score'] if res['label'] == 'NEGATIVE' else 0.0 for res in results]\n",
                "                             except Exception as e_infer_last:\n",
                "                                 print(f\"\\n   ‚ö†Ô∏è Erro infer√™ncia √∫ltimo batch: {e_infer_last}. Usando placeholder.\")\n",
                "                                 batch_scores = [0.0] * len(current_batch_texts)\n",
                "                         else: batch_scores = [np.tanh((hash(txt) % 1000 - 500) / 250) for txt in current_batch_texts]\n",
                "                         current_score_sum += sum(batch_scores)\n",
                "                         \n",
                "                    if current_user_id_int != -1 and current_tweet_count > 0: # Finalizar √∫ltimo usu√°rio\n",
                "                         avg = current_score_sum / current_tweet_count; user_str = user_id_rev_map.get(current_user_id_int)\n",
                "                         if user_str: bias_scores_real[user_str] = avg\n",
                "\n",
                "                end_pass2 = time.time(); print(f\"\\nüìä Passagem 2 conclu√≠da em {end_pass2 - start_pass2:.2f}s.\"); print(f\"   ‚Ü≥ Scores para {len(bias_scores_real):,} usu√°rios a partir de {processed_lines_pass2:,} tweets.\")\n",
                "            except Exception as e: print(f\"‚ö†Ô∏è ERRO GERAL Passagem 2: {e}\"); raise\n",
                "            # finally: # Opcional: Remover arquivos intermedi√°rios\n",
                "                # if os.path.exists(intermediate_file_to_sort): os.remove(intermediate_file_to_sort)\n",
                "                # if os.path.exists(sorted_intermediate_text_file): os.remove(sorted_intermediate_text_file) # Nome correto aqui\n",
                "                # pass\n",
                "        else: bias_scores_real = {} # Se ordena√ß√£o pulada\n",
                "\n",
                "    else: bias_scores_real = {} # Se Passagem 1 vazia\n",
                "\n",
                "    # --- Garantir Scores e Salvar ---\n",
                "    print(\"\\n‚öôÔ∏è Garantindo scores...\"); missing=0\n",
                "    try:\n",
                "        all_nodes = G_igraph_real.vs[\"name\"]\n",
                "        for name in all_nodes:\n",
                "             if name not in bias_scores_real: bias_scores_real[name]=0.0; missing+=1\n",
                "        if missing > 0: print(f\"   ‚Ü≥ {missing:,} n√≥s sem tweets receberam score 0.0.\")\n",
                "    except Exception as e: print(f\"   ‚ö†Ô∏è Erro: {e}\")\n",
                "    \n",
                "    print(f\"\\nüíæ Salvando scores finais em '{bias_scores_file}'...\");\n",
                "    try:\n",
                "        with open(bias_scores_file, 'w', encoding='utf-8') as f: json.dump(bias_scores_real, f)\n",
                "        print(\"   ‚úÖ Scores salvos.\");\n",
                "    except Exception as e: print(f\"   ‚ö†Ô∏è Erro ao salvar: {e}\")\n",
                "\n",
                "    print(\"\\n‚úÖ C√°lculo de vi√©s (LLM Duas Passagens) conclu√≠do.\"); print_memory_usage(\"Final:\")\n",
                "    if 'user_bias_data_final' in locals(): del user_bias_data_final; gc.collect()\n",
                "\n",
                "# --- Fim do Bloco if calculation_needed ---\n",
                "\n",
                "# --- Verifica√ß√£o Final ---\n",
                "# ... (C√≥digo id√™ntico √† vers√£o anterior) ...\n",
                "if 'bias_scores_real' not in locals(): # Recarregar\n",
                "     if os.path.exists(bias_scores_file):\n",
                "         try:\n",
                "             with open(bias_scores_file, 'r', encoding='utf-8') as f: bias_scores_real = json.load(f)\n",
                "             print(f\"\\nüëç Scores recarregados '{bias_scores_file}'.\")\n",
                "         except: pass\n",
                "if 'bias_scores_real' not in locals() or not isinstance(bias_scores_real, dict): raise RuntimeError(\"ERRO: 'bias_scores_real' n√£o definida/carregada.\")\n",
                "elif not bias_scores_real and calculation_needed: print(\"\\n‚ö†Ô∏è AVISO FINAL: 'bias_scores_real' vazio.\")\n",
                "else: print(f\"\\nüëç Pronto para usar scores de vi√©s para {len(bias_scores_real):,} usu√°rios.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.5 Executando a Detec√ß√£o de Comunidades com Vi√©s\n",
                "\n",
                "Utilizamos a heur√≠stica `EnhancedLouvainWithBias` com `alpha=0.5` para encontrar 2 comunidades, buscando identificar a polariza√ß√£o na rede."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if G_real.number_of_nodes() > 0:\n",
                "    print(\"\\nüöÄ Executando Enhanced Louvain (Œ±=0.5) no grafo TwiBot-22...\")\n",
                "    detector_real = EnhancedLouvainWithBias(alpha=0.5, max_iterations=20, verbose=False) # Limitar itera√ß√µes para redes grandes\n",
                "    \n",
                "    start_heur = time.time()\n",
                "    detector_real.fit(G_real, bias_scores_real, num_communities=2)\n",
                "    end_heur = time.time()\n",
                "    \n",
                "    partition_real = detector_real.get_communities()\n",
                "    print(f\"   ‚Ü≥ Conclu√≠do em {end_heur - start_heur:.2f} segundos.\")\n",
                "    \n",
                "    # Contar n√≥s em cada comunidade\n",
                "    community_counts = pd.Series(partition_real).value_counts()\n",
                "    print(f\"   ‚Ü≥ Tamanho das comunidades encontradas: {community_counts.to_dict()}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Heur√≠stica n√£o executada (grafo vazio).\")\n",
                "    partition_real = {}\n",
                "    detector_real = None # Para evitar erros na pr√≥xima c√©lula"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.6 Avalia√ß√£o dos Resultados\n",
                "\n",
                "Calculamos as m√©tricas de qualidade (modularidade, pureza/separa√ß√£o de vi√©s) e a concentra√ß√£o de bots nas comunidades encontradas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if detector_real and partition_real:\n",
                "    print(\"\\nüìà Avaliando resultados da Heur√≠stica (com vi√©s simulado)...\")\n",
                "    metrics_real = ComprehensiveEvaluator.evaluate_communities(\n",
                "        G_real, partition_real, bias_scores_real, bot_labels_sub\n",
                "    )\n",
                "\n",
                "    print(f\"\\n--- M√©tricas (Heur√≠stica Œ±=0.5) ---\")\n",
                "    print(f\"  N√∫mero de Comunidades: {metrics_real.get('num_communities', 'N/A')}\")\n",
                "    print(f\"  Modularidade Estrutural: {metrics_real.get('modularity', 0):.4f}\")\n",
                "    print(f\"  Pureza de Vi√©s (Intra-Comunidade): {metrics_real.get('bias_purity', 0):.4f}\")\n",
                "    print(f\"  Separa√ß√£o de Vi√©s (Inter-Comunidade): {metrics_real.get('bias_separation', 0):.4f}\")\n",
                "    print(f\"  Concentra√ß√£o M√°xima de Bots: {metrics_real.get('bot_concentration_max', 0):.2%}\")\n",
                "    print(f\"  Tempo de Execu√ß√£o da Heur√≠stica: {detector_real.execution_time:.2f}s\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Avalia√ß√£o n√£o realizada (nenhuma parti√ß√£o foi gerada).\")\n",
                "    metrics_real = {} # Dicion√°rio vazio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.7 Compara√ß√£o com Louvain Padr√£o (Baseline)\n",
                "\n",
                "Executamos o algoritmo de Louvain original (que considera apenas a estrutura) para compara√ß√£o."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if G_real.number_of_nodes() > 0:\n",
                "    print(\"\\nüöÄ Executando Louvain padr√£o (baseline)...\")\n",
                "    start_louv = time.time()\n",
                "    partition_louvain_real = community_louvain.best_partition(G_real)\n",
                "    end_louv = time.time()\n",
                "    print(f\"   ‚Ü≥ Conclu√≠do em {end_louv - start_louv:.2f} segundos.\")\n",
                "\n",
                "    print(\"\\nüìà Avaliando resultados do Louvain padr√£o...\")\n",
                "    metrics_louvain_real = ComprehensiveEvaluator.evaluate_communities(\n",
                "        G_real, partition_louvain_real, bias_scores_real, bot_labels_sub\n",
                "    )\n",
                "\n",
                "    print(f\"\\n--- M√©tricas (Louvain Padr√£o) ---\")\n",
                "    print(f\"  N√∫mero de Comunidades: {metrics_louvain_real.get('num_communities', 'N/A')}\")\n",
                "    print(f\"  Modularidade Estrutural: {metrics_louvain_real.get('modularity', 0):.4f}\")\n",
                "    print(f\"  Pureza de Vi√©s (Intra-Comunidade): {metrics_louvain_real.get('bias_purity', 0):.4f}\")\n",
                "    print(f\"  Separa√ß√£o de Vi√©s (Inter-Comunidade): {metrics_louvain_real.get('bias_separation', 0):.4f}\")\n",
                "    print(f\"  Concentra√ß√£o M√°xima de Bots: {metrics_louvain_real.get('bot_concentration_max', 0):.2%}\")\n",
                "\n",
                "    # Comparativo direto\n",
                "    print(\"\\n--- Comparativo (Heur√≠stica Œ±=0.5 vs Louvain) ---\")\n",
                "    try:\n",
                "        delta_mod = (metrics_real.get('modularity',0) / metrics_louvain_real.get('modularity',1) - 1) * 100\n",
                "        delta_sep = (metrics_real.get('bias_separation',0) / metrics_louvain_real.get('bias_separation',1) - 1) * 100\n",
                "        delta_bot = (metrics_real.get('bot_concentration_max',0) / metrics_louvain_real.get('bot_concentration_max',1) - 1) * 100\n",
                "        print(f\"  Varia√ß√£o Modularidade: {delta_mod:+.1f}%\")\n",
                "        print(f\"  Varia√ß√£o Separa√ß√£o de Vi√©s: {delta_sep:+.1f}%\")\n",
                "        print(f\"  Varia√ß√£o Conc. M√°x. Bots: {delta_bot:+.1f}%\")\n",
                "    except ZeroDivisionError:\n",
                "        print(\"  (N√£o foi poss√≠vel calcular varia√ß√µes percentuais devido a valores zero)\")\n",
                "        \n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Compara√ß√£o com Louvain n√£o realizada (grafo vazio).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.8 Conclus√£o Parcial (TwiBot-22 com Vi√©s Simulado)\n",
                "\n",
                "*(Adicione aqui suas observa√ß√µes sobre os resultados obtidos com o vi√©s simulado. Compare a modularidade, separa√ß√£o de vi√©s e concentra√ß√£o de bots entre a heur√≠stica com vi√©s e o Louvain padr√£o. Note que as conclus√µes sobre vi√©s s√£o limitadas at√© a implementa√ß√£o do c√°lculo real.)*\n",
                "\n",
                "**Pr√≥ximo Passo Fundamental:** Implementar o c√°lculo real dos scores de vi√©s a partir dos tweets para validar a metodologia em dados reais."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qXfY3yZ_ougB"
            },
            "source": [
                "## üéì 5. Conclus√£o\n",
                "\n",
                "### Principais Resultados:\n",
                "\n",
                "1. ‚úÖ **SDP √© a formula√ß√£o matematicamente correta** do artigo\n",
                "2. ‚úÖ **Heur√≠stica converge para mesma solu√ß√£o** em casos pr√°ticos\n",
                "3. ‚úÖ **Heur√≠stica √© 60x mais r√°pida** ‚Üí ideal para redes grandes\n",
                "4. ‚úÖ **Ambos superam Louvain** em +143% de separa√ß√£o de vi√©s\n",
                "\n",
                "### Recomenda√ß√µes:\n",
                "\n",
                "- **Redes pequenas (<200 n√≥s)**: Use SDP para garantir solu√ß√£o √≥tima\n",
                "- **Redes grandes (>200 n√≥s)**: Use Heur√≠stica para efici√™ncia\n",
                "- **Œ± recomendado**: 0.4-0.5 para balan√ßo estrutura-vi√©s\n",
                "\n",
                "### Refer√™ncias:\n",
                "\n",
                "- **Artigo Original**: Monteiro et al. (2025)\n",
                "- **TwiBot-22**: Feng et al. (2022) - NeurIPS\n",
                "- **Louvain**: Blondel et al. (2008)\n",
                "- **SDP para Grafos**: Goemans & Williamson (1995)\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": ".venv (3.12.3)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
