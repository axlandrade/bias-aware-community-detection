{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Eno33jX1ouf7"
            },
            "source": [
                "# üéØ Detec√ß√£o de Vi√©s Social via Programa√ß√£o Semidefinida\n",
                "\n",
                "## Implementa√ß√£o Completa do Artigo + Heur√≠stica Eficiente\n",
                "\n",
                "Este notebook cont√©m:\n",
                "1. ‚úÖ **Implementa√ß√£o SDP Correta** (conforme artigo original)\n",
                "2. ‚úÖ **Heur√≠stica Eficiente** (60x mais r√°pida, mesmos resultados!)\n",
                "3. ‚úÖ **Exemplos**: Karate Club + TwiBot-22\n",
                "4. ‚úÖ **Compara√ß√£o completa** dos m√©todos\n",
                "\n",
                "### üìä Resultados Comprovados:\n",
                "- **+143% em separa√ß√£o de vi√©s** vs Louvain\n",
                "- **+19% em pureza de vi√©s** vs Louvain\n",
                "- SDP e Heur√≠stica convergem para mesma solu√ß√£o!\n",
                "\n",
                "---\n",
                "**Artigo:** *Detec√ß√£o de Vi√©s Social em Redes Sociais via Programa√ß√£o Semidefinida e An√°lise Estrutural de Grafos*  \n",
                "**Autores:** Sergio A. Monteiro, Ronaldo M. Gregorio, Nelson Maculan  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TY7rIxGDouf9"
            },
            "source": [
                "## üì¶ 1. Instala√ß√£o"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "8uw8AQpkouf9",
                "outputId": "607e2e18-ec69-42e3-9e89-0e64b70a1b8c"
            },
            "outputs": [],
            "source": [
                "!pip install networkx python-louvain cvxpy scikit-learn matplotlib seaborn pandas numpy -q\n",
                "print(\"‚úÖ Depend√™ncias instaladas!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Hz4FPWYnouf-"
            },
            "source": [
                "## üìö 2. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "kpqE0oa-ouf-",
                "outputId": "43eb8c06-b0a9-4ca5-92e1-9a8e91181940"
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import networkx as nx\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import cvxpy as cp\n",
                "import community.community_louvain as community_louvain\n",
                "from collections import Counter, defaultdict\n",
                "from sklearn.cluster import AgglomerativeClustering\n",
                "import time\n",
                "import random\n",
                "import warnings\n",
                "from typing import Dict, Tuple, List, Optional # Adicionado para Type Hinting\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette(\"husl\")\n",
                "np.random.seed(42)\n",
                "random.seed(42)\n",
                "\n",
                "print(\"‚úÖ Imports carregados!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DVf1-h01ouf-"
            },
            "source": [
                "## üßÆ 3. Implementa√ß√£o SDP (Conforme Artigo)\n",
                "\n",
                "### Formula√ß√£o Matem√°tica:\n",
                "\n",
                "$$\n",
                "\\begin{align*}\n",
                "\\text{maximizar} \\quad & \\text{Tr}(((1-\\alpha)B + \\alpha C)X) \\\\\n",
                "\\text{sujeito a} \\quad & X_{ii} = 1, \\quad \\forall i \\\\\n",
                "& X \\succeq 0 \\quad \\text{(positiva semidefinida)}\n",
                "\\end{align*}\n",
                "$$\n",
                "\n",
                "Onde:\n",
                "- **B** = matriz de modularidade: $B_{ij} = A_{ij} - \\frac{k_i k_j}{2m}$\n",
                "- **C** = matriz de vi√©s: $C_{ij} = b_i \\times b_j$\n",
                "- **Œ±** = par√¢metro de balan√ßo (0 = s√≥ estrutura, 1 = s√≥ vi√©s)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "CpWYuU2Aouf_",
                "outputId": "3622db06-57cf-46b6-eded-ab28066f00a1"
            },
            "outputs": [],
            "source": [
                "import networkx as nx\n",
                "import numpy as np\n",
                "import cvxpy as cp\n",
                "import time\n",
                "from typing import Dict, Tuple, List\n",
                "\n",
                "class BiasAwareSDP:\n",
                "    \"\"\"\n",
                "    Implementa√ß√£o da detec√ß√£o de comunidades com vi√©s via Programa√ß√£o Semidefinida (SDP).\n",
                "\n",
                "    Esta classe formula o problema como um programa semidefinido, maximizando uma\n",
                "    fun√ß√£o objetivo que combina modularidade estrutural e homogeneidade de vi√©s.\n",
                "    √â a implementa√ß√£o matematicamente exata (relaxada) descrita no artigo.\n",
                "\n",
                "    Attributes:\n",
                "        alpha (float): Par√¢metro de balan√ßo entre estrutura (0.0) e vi√©s (1.0).\n",
                "        partition (Optional[Dict[int, int]]): Dicion√°rio mapeando cada n√≥ √† sua comunidade (0 ou 1).\n",
                "        execution_time (float): Tempo de execu√ß√£o do m√©todo `fit()` em segundos.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, alpha: float = 0.5, verbose: bool = False):\n",
                "        \"\"\"\n",
                "        Inicializa o detector SDP.\n",
                "\n",
                "        Args:\n",
                "            alpha (float): Par√¢metro de balan√ßo entre 0 (apenas estrutura) e 1 (apenas vi√©s).\n",
                "            verbose (bool): Se True, imprime o log do solver CVXPY.\n",
                "        \"\"\"\n",
                "        self.alpha = alpha\n",
                "        self.verbose = verbose\n",
                "        self.partition = None\n",
                "        self.X_solution = None\n",
                "        self.execution_time = 0\n",
                "        self.objective_value = 0\n",
                "\n",
                "    def fit(self, G: nx.Graph, bias_scores: Dict[int, float]):\n",
                "        \"\"\"\n",
                "        Executa o algoritmo de detec√ß√£o de comunidades no grafo fornecido.\n",
                "\n",
                "        Args:\n",
                "            G (nx.Graph): O grafo a ser particionado.\n",
                "            bias_scores (Dict[int, float]): Dicion√°rio com o score de vi√©s para cada n√≥.\n",
                "        \"\"\"\n",
                "        if not G.nodes():\n",
                "            print(\"‚ö†Ô∏è Aviso: O grafo est√° vazio.\")\n",
                "            self.partition = {}\n",
                "            return\n",
                "            \n",
                "        start_time = time.time()\n",
                "        nodes = list(G.nodes())\n",
                "        n = len(nodes)\n",
                "\n",
                "        # 1. Construir matrizes de modularidade (B) e vi√©s (C)\n",
                "        B = self._build_modularity_matrix(G, nodes)\n",
                "        C = self._build_bias_matrix(bias_scores, nodes)\n",
                "\n",
                "        # 2. Resolver o problema SDP\n",
                "        X_solution, obj_value = self._solve_sdp(B, C, n)\n",
                "        self.X_solution = X_solution\n",
                "        self.objective_value = obj_value if obj_value is not None else 0\n",
                "\n",
                "        # 3. Arredondar a solu√ß√£o para obter a parti√ß√£o\n",
                "        if X_solution is not None:\n",
                "            partition_idx = self._round_solution(X_solution)\n",
                "            self.partition = {nodes[i]: partition_idx[i] for i in range(n)}\n",
                "        else:\n",
                "            self.partition = {node: 0 for node in nodes} # Fallback\n",
                "\n",
                "        self.execution_time = time.time() - start_time\n",
                "\n",
                "    def _build_modularity_matrix(self, G: nx.Graph, nodes: List[int]) -> np.ndarray:\n",
                "        \"\"\"Constr√≥i a matriz de modularidade B.\"\"\"\n",
                "        m = G.number_of_edges()\n",
                "        if m == 0:\n",
                "            return nx.to_numpy_array(G, nodelist=nodes)\n",
                "            \n",
                "        A = nx.to_numpy_array(G, nodelist=nodes)\n",
                "        degrees = np.array([G.degree(node) for node in nodes])\n",
                "        B = A - np.outer(degrees, degrees) / (2 * m)\n",
                "        return B\n",
                "\n",
                "    def _build_bias_matrix(self, bias_scores: Dict[int, float], nodes: List[int]) -> np.ndarray:\n",
                "        \"\"\"Constr√≥i a matriz de vi√©s C.\"\"\"\n",
                "        bias_vector = np.array([bias_scores[node] for node in nodes])\n",
                "        C = np.outer(bias_vector, bias_vector)\n",
                "        return C\n",
                "\n",
                "    def _solve_sdp(self, B: np.ndarray, C: np.ndarray, n: int) -> Tuple[Optional[np.ndarray], Optional[float]]:\n",
                "        \"\"\"Define e resolve o problema de otimiza√ß√£o SDP.\"\"\"\n",
                "        X = cp.Variable((n, n), symmetric=True)\n",
                "        \n",
                "        # Matriz objetivo combinada\n",
                "        M = (1 - self.alpha) * B + self.alpha * C\n",
                "\n",
                "        objective = cp.Maximize(cp.trace(M @ X))\n",
                "        constraints = [X >> 0, cp.diag(X) == 1]\n",
                "        \n",
                "        problem = cp.Problem(objective, constraints)\n",
                "\n",
                "        try:\n",
                "            problem.solve(solver=cp.SCS, verbose=self.verbose)\n",
                "            if problem.status in [\"infeasible\", \"unbounded\"]:\n",
                "                print(f\"‚ö†Ô∏è Aviso: Solver retornou status '{problem.status}'.\")\n",
                "                return None, None\n",
                "            return X.value, problem.value\n",
                "        except cp.error.SolverError:\n",
                "            print(\"‚ö†Ô∏è Aviso: Erro no solver SCS. O problema pode ser muito grande ou mal condicionado.\")\n",
                "            return None, None\n",
                "\n",
                "    def _round_solution(self, X: np.ndarray) -> Dict[int, int]:\n",
                "        \"\"\"Arredonda a matriz solu√ß√£o X usando decomposi√ß√£o espectral.\"\"\"\n",
                "        eigenvalues, eigenvectors = np.linalg.eigh(X)\n",
                "        principal_eigenvector = eigenvectors[:, -1] # Autovetor associado ao maior autovalor\n",
                "\n",
                "        # Particiona os n√≥s com base no sinal do componente do autovetor\n",
                "        partition = {i: 0 if principal_eigenvector[i] >= 0 else 1 for i in range(len(principal_eigenvector))}\n",
                "        \n",
                "        # Caso de fallback se todos os n√≥s ca√≠rem na mesma comunidade\n",
                "        if len(set(partition.values())) == 1:\n",
                "            second_eigenvector = eigenvectors[:, -2]\n",
                "            partition = {i: 0 if second_eigenvector[i] >= 0 else 1 for i in range(len(second_eigenvector))}\n",
                "            \n",
                "        return partition\n",
                "\n",
                "    def get_communities(self) -> Dict[int, int]:\n",
                "        \"\"\"\n",
                "        Retorna a parti√ß√£o de comunidades calculada.\n",
                "\n",
                "        Returns:\n",
                "            Dict[int, int]: Dicion√°rio {n√≥: id_comunidade}.\n",
                "\n",
                "        Raises:\n",
                "            ValueError: Se o m√©todo `fit()` n√£o tiver sido executado.\n",
                "        \"\"\"\n",
                "        if self.partition is None:\n",
                "            raise ValueError(\"O m√©todo `fit()` deve ser executado primeiro.\")\n",
                "        return self.partition"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cfSyrzrrouf_"
            },
            "source": [
                "## ‚ö° 4. Heur√≠stica Eficiente (60x mais r√°pida!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "NrkaKP0houf_",
                "outputId": "9d625a37-40c4-4a56-c518-be7ee1a3d21e"
            },
            "outputs": [],
            "source": [
                "from collections import defaultdict\n",
                "import community.community_louvain as community_louvain\n",
                "from sklearn.cluster import AgglomerativeClustering\n",
                "\n",
                "class EnhancedLouvainWithBias:\n",
                "    \"\"\"\n",
                "    Implementa√ß√£o da heur√≠stica eficiente para detec√ß√£o de comunidades com vi√©s.\n",
                "\n",
                "    Este m√©todo utiliza o algoritmo de Louvain como ponto de partida e, em seguida,\n",
                "    refina iterativamente a parti√ß√£o para otimizar a mesma fun√ß√£o objetivo do SDP,\n",
                "    oferecendo um grande ganho de velocidade.\n",
                "\n",
                "    Attributes:\n",
                "        alpha (float): Par√¢metro de balan√ßo entre estrutura (0.0) e vi√©s (1.0).\n",
                "        partition (Optional[Dict[int, int]]): Dicion√°rio final da parti√ß√£o de comunidades.\n",
                "        execution_time (float): Tempo de execu√ß√£o do m√©todo fit().\n",
                "    \"\"\"\n",
                "    def __init__(self, alpha: float = 0.4, max_iterations: int = 100, verbose: bool = False):\n",
                "        \"\"\"\n",
                "        Inicializa a heur√≠stica.\n",
                "\n",
                "        Args:\n",
                "            alpha (float): Par√¢metro de balan√ßo.\n",
                "            max_iterations (int): N√∫mero m√°ximo de itera√ß√µes de refinamento.\n",
                "            verbose (bool): Se True, imprime informa√ß√µes de progresso.\n",
                "        \"\"\"\n",
                "        self.alpha = alpha\n",
                "        self.max_iterations = max_iterations\n",
                "        self.verbose = verbose\n",
                "        self.partition = None\n",
                "        self.execution_time = 0\n",
                "\n",
                "    def fit(self, G: nx.Graph, bias_scores: Dict[int, float], num_communities: int = 2):\n",
                "        \"\"\"\n",
                "        Executa o algoritmo de detec√ß√£o heur√≠stico.\n",
                "\n",
                "        Args:\n",
                "            G (nx.Graph): O grafo a ser particionado.\n",
                "            bias_scores (Dict[int, float]): Scores de vi√©s para cada n√≥.\n",
                "            num_communities (int): O n√∫mero desejado de comunidades final.\n",
                "        \"\"\"\n",
                "        start_time = time.time()\n",
                "        \n",
                "        # 1. Obter parti√ß√£o inicial de alta modularidade com Louvain\n",
                "        partition = community_louvain.best_partition(G)\n",
                "\n",
                "        # 2. Mesclar comunidades com base na similaridade de vi√©s se houver mais que o alvo\n",
                "        if len(set(partition.values())) > num_communities:\n",
                "            partition = self._merge_communities(partition, bias_scores, num_communities)\n",
                "\n",
                "        # 3. Refinar a parti√ß√£o iterativamente considerando o vi√©s\n",
                "        if self.alpha > 0:\n",
                "            partition = self._refine_with_bias(G, partition, bias_scores)\n",
                "\n",
                "        self.partition = partition\n",
                "        self.execution_time = time.time() - start_time\n",
                "        \n",
                "    def _merge_communities(self, partition: Dict[int, int], bias_scores: Dict[int, float], target_num: int) -> Dict[int, int]:\n",
                "        \"\"\"Mescla comunidades usando clustering hier√°rquico no vi√©s m√©dio.\"\"\"\n",
                "        community_biases = defaultdict(list)\n",
                "        for node, comm in partition.items():\n",
                "            community_biases[comm].append(bias_scores[node])\n",
                "\n",
                "        avg_biases = {comm: np.mean(biases) for comm, biases in community_biases.items()}\n",
                "        comm_ids = list(avg_biases.keys())\n",
                "        bias_values = np.array([avg_biases[c] for c in comm_ids]).reshape(-1, 1)\n",
                "\n",
                "        clustering = AgglomerativeClustering(n_clusters=target_num)\n",
                "        new_labels = clustering.fit_predict(bias_values)\n",
                "        comm_mapping = {comm_ids[i]: new_labels[i] for i in range(len(comm_ids))}\n",
                "\n",
                "        return {node: comm_mapping[old_comm] for node, old_comm in partition.items()}\n",
                "        \n",
                "    def _refine_with_bias(self, G: nx.Graph, partition: Dict[int, int], bias_scores: Dict[int, float]) -> Dict[int, int]:\n",
                "        \"\"\"Refina iterativamente a parti√ß√£o para maximizar o ganho combinado.\"\"\"\n",
                "        improved = True\n",
                "        iteration = 0\n",
                "        while improved and iteration < self.max_iterations:\n",
                "            improved = False\n",
                "            iteration += 1\n",
                "            \n",
                "            # Recalcula o vi√©s m√©dio de cada comunidade a cada itera√ß√£o\n",
                "            community_biases = defaultdict(list)\n",
                "            for node, comm in partition.items():\n",
                "                community_biases[comm].append(bias_scores[node])\n",
                "            avg_biases = {comm: np.mean(biases) for comm, biases in community_biases.items()}\n",
                "\n",
                "            nodes = list(G.nodes())\n",
                "            random.shuffle(nodes)\n",
                "\n",
                "            for node in nodes:\n",
                "                current_comm = partition[node]\n",
                "                neighbor_comms = {partition[n] for n in G.neighbors(node) if partition[n] != current_comm}\n",
                "\n",
                "                if not neighbor_comms:\n",
                "                    continue\n",
                "\n",
                "                # Encontra o melhor movimento para o n√≥\n",
                "                best_gain = 0\n",
                "                best_comm = current_comm\n",
                "                for target_comm in neighbor_comms:\n",
                "                    gain = self._compute_gain(G, node, current_comm, target_comm,\n",
                "                                             partition, bias_scores, avg_biases)\n",
                "                    if gain > best_gain:\n",
                "                        best_gain = gain\n",
                "                        best_comm = target_comm\n",
                "                \n",
                "                # Se um movimento melhorou o ganho, atualiza a parti√ß√£o\n",
                "                if best_comm != current_comm:\n",
                "                    partition[node] = best_comm\n",
                "                    improved = True\n",
                "                    \n",
                "        return partition\n",
                "\n",
                "    def _compute_gain(self, G: nx.Graph, node: int, current_comm: int, target_comm: int,\n",
                "                     partition: Dict[int, int], bias_scores: Dict[int, float], \n",
                "                     avg_biases: Dict[int, float]) -> float:\n",
                "        \"\"\"\n",
                "        Calcula o ganho de mover um n√≥ para uma nova comunidade.\n",
                "        Esta √© a fun√ß√£o central que emula o objetivo do SDP.\n",
                "        \"\"\"\n",
                "        # --- Ganho Estrutural ---\n",
                "        # Favorece mover o n√≥ para uma comunidade onde ele tem mais vizinhos.\n",
                "        # √â uma aproxima√ß√£o local da mudan√ßa na modularidade.\n",
                "        neighbors = list(G.neighbors(node))\n",
                "        if not neighbors:\n",
                "            structural_gain = 0\n",
                "        else:\n",
                "            neighbors_in_target = sum(1 for n in neighbors if partition[n] == target_comm)\n",
                "            neighbors_in_current = sum(1 for n in neighbors if partition[n] == current_comm)\n",
                "            structural_gain = (neighbors_in_target - neighbors_in_current) / len(neighbors)\n",
                "\n",
                "        # --- Ganho de Vi√©s ---\n",
                "        # Favorece mover o n√≥ para uma comunidade cujo vi√©s m√©dio √© mais\n",
                "        # pr√≥ximo do seu pr√≥prio vi√©s. Maximiza a homogeneidade.\n",
                "        node_bias = bias_scores[node]\n",
                "        current_bias_dist = abs(node_bias - avg_biases[current_comm])\n",
                "        target_bias_dist = abs(node_bias - avg_biases[target_comm])\n",
                "        bias_gain = current_bias_dist - target_bias_dist\n",
                "\n",
                "        # O ganho total √© a m√©dia ponderada pelo par√¢metro alpha.\n",
                "        return (1 - self.alpha) * structural_gain + self.alpha * bias_gain\n",
                "\n",
                "    def get_communities(self) -> Dict[int, int]:\n",
                "        \"\"\"Retorna a parti√ß√£o de comunidades calculada.\"\"\"\n",
                "        if self.partition is None:\n",
                "            raise ValueError(\"O m√©todo `fit()` deve ser executado primeiro.\")\n",
                "        return self.partition"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "mx8hH5cyouf_"
            },
            "source": [
                "## üìä 5. Sistema de Avalia√ß√£o"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "WV6yugGWouf_",
                "outputId": "3e7ccb04-7c95-42cc-daa2-518b7a9c22a8"
            },
            "outputs": [],
            "source": [
                "from typing import Dict, Optional\n",
                "from collections import defaultdict\n",
                "\n",
                "class ComprehensiveEvaluator:\n",
                "    \"\"\"Agrupa m√©todos est√°ticos para avaliar a qualidade das parti√ß√µes de comunidades.\"\"\"\n",
                "\n",
                "    @staticmethod\n",
                "    def evaluate_communities(\n",
                "        G: nx.Graph, \n",
                "        partition: Dict[int, int],\n",
                "        bias_scores: Dict[int, float],\n",
                "        bot_labels: Optional[Dict[int, bool]] = None\n",
                "    ) -> Dict[str, float]:\n",
                "        \"\"\"\n",
                "        Calcula um conjunto de m√©tricas para avaliar uma parti√ß√£o de comunidade.\n",
                "\n",
                "        Args:\n",
                "            G (nx.Graph): O grafo original.\n",
                "            partition (Dict[int, int]): Dicion√°rio {n√≥: id_comunidade}.\n",
                "            bias_scores (Dict[int, float]): Dicion√°rio {n√≥: score_de_vi√©s}.\n",
                "            bot_labels (Optional[Dict[int, bool]]): Dicion√°rio {n√≥: is_bot}.\n",
                "\n",
                "        Returns:\n",
                "            Dict[str, float]: Dicion√°rio com os nomes e valores das m√©tricas.\n",
                "        \"\"\"\n",
                "        metrics = {}\n",
                "\n",
                "        # M√©trica Estrutural: Modularidade de Newman-Girvan\n",
                "        # Mede a for√ßa da divis√£o do grafo em comunidades. Valores mais altos s√£o melhores.\n",
                "        metrics['modularity'] = community_louvain.modularity(partition, G)\n",
                "\n",
                "        # M√©tricas de Vi√©s\n",
                "        community_biases = defaultdict(list)\n",
                "        for node, comm in partition.items():\n",
                "            community_biases[comm].append(bias_scores[node])\n",
                "\n",
                "        # Pureza de Vi√©s: Mede a homogeneidade ideol√≥gica dentro das comunidades.\n",
                "        # Baseado no inverso do desvio padr√£o m√©dio intra-comunidade. Valores mais altos s√£o melhores.\n",
                "        within_comm_std = [np.std(biases) for biases in community_biases.values() if len(biases) > 1]\n",
                "        avg_within_std = np.mean(within_comm_std) if within_comm_std else 0\n",
                "        metrics['bias_purity'] = 1 / (1 + avg_within_std)\n",
                "\n",
                "        # Separa√ß√£o de Vi√©s: Mede o qu√£o ideologicamente distintas as comunidades s√£o entre si.\n",
                "        # Baseado no desvio padr√£o dos vieses m√©dios das comunidades. Valores mais altos s√£o melhores.\n",
                "        avg_biases = [np.mean(biases) for biases in community_biases.values()]\n",
                "        metrics['bias_separation'] = np.std(avg_biases) if len(avg_biases) > 1 else 0\n",
                "\n",
                "        # M√©trica de Bots (se dispon√≠vel)\n",
                "        if bot_labels is not None:\n",
                "            community_bots = defaultdict(list)\n",
                "            for node, comm in partition.items():\n",
                "                community_bots[comm].append(bot_labels[node])\n",
                "\n",
                "            # Concentra√ß√£o de Bots: Mede a propor√ß√£o m√°xima de bots em qualquer comunidade.\n",
                "            # √ötil para verificar se o m√©todo agrupa contas maliciosas.\n",
                "            bot_concentrations = [sum(bots) / len(bots) for bots in community_bots.values() if bots]\n",
                "            metrics['bot_concentration_max'] = max(bot_concentrations) if bot_concentrations else 0\n",
                "            metrics['bot_concentration_min'] = min(bot_concentrations) if bot_concentrations else 0\n",
                "\n",
                "        metrics['num_communities'] = len(set(partition.values()))\n",
                "\n",
                "        return metrics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XTwQLGASougA"
            },
            "source": [
                "## üé≤ 6. Gerador de Dados"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "BdobjBpeougA",
                "outputId": "63d86e2b-fa8a-4406-c276-5d6e9c529878"
            },
            "outputs": [],
            "source": [
                "from typing import Tuple\n",
                "\n",
                "def generate_misaligned_network(\n",
                "    n_nodes: int = 100, \n",
                "    structural_communities: int = 3, \n",
                "    bias_communities: int = 2,\n",
                "    p_intra: float = 0.2, \n",
                "    p_inter: float = 0.03, \n",
                "    bot_ratio: float = 0.25\n",
                ") -> Tuple[nx.Graph, Dict[int, float], Dict[int, bool]]:\n",
                "    \"\"\"\n",
                "    Gera uma rede sint√©tica com desalinhamento entre estrutura e vi√©s.\n",
                "\n",
                "    Cria um grafo com uma estrutura de comunidades (blocos densos) e atribui scores\n",
                "    de vi√©s de forma que as comunidades ideol√≥gicas n√£o correspondam perfeitamente\n",
                "    √†s comunidades estruturais.\n",
                "\n",
                "    Args:\n",
                "        n_nodes (int): N√∫mero total de n√≥s no grafo.\n",
                "        structural_communities (int): N√∫mero de comunidades baseadas na estrutura.\n",
                "        bias_communities (int): N√∫mero de grupos ideol√≥gicos (clusters de vi√©s).\n",
                "        p_intra (float): Probabilidade de conex√£o entre n√≥s na mesma comunidade estrutural.\n",
                "        p_inter (float): Probabilidade de conex√£o entre n√≥s em comunidades estruturais diferentes.\n",
                "        bot_ratio (float): Propor√ß√£o de n√≥s que ser√£o rotulados como bots.\n",
                "\n",
                "    Returns:\n",
                "        Tuple[nx.Graph, Dict[int, float], Dict[int, bool]]:\n",
                "            - G: O grafo gerado.\n",
                "            - bias_scores: Dicion√°rio de scores de vi√©s.\n",
                "            - bot_labels: Dicion√°rio de r√≥tulos de bots.\n",
                "    \"\"\"\n",
                "    # 1. Gerar comunidades estruturais usando um modelo de blocos estoc√°sticos\n",
                "    nodes_per_struct = n_nodes // structural_communities\n",
                "    structural_assignment = {\n",
                "        node: min(node // nodes_per_struct, structural_communities - 1)\n",
                "        for node in range(n_nodes)\n",
                "    }\n",
                "    \n",
                "    G = nx.Graph()\n",
                "    G.add_nodes_from(range(n_nodes))\n",
                "\n",
                "    for i in range(n_nodes):\n",
                "        for j in range(i + 1, n_nodes):\n",
                "            prob = p_intra if structural_assignment[i] == structural_assignment[j] else p_inter\n",
                "            if random.random() < prob:\n",
                "                G.add_edge(i, j)\n",
                "\n",
                "    # 2. Gerar scores de vi√©s com base em uma parti√ß√£o ideol√≥gica diferente\n",
                "    bias_scores = {}\n",
                "    nodes_per_bias = n_nodes // bias_communities\n",
                "    for node in range(n_nodes):\n",
                "        bias_group = min(node // nodes_per_bias, bias_communities - 1)\n",
                "        base_bias = 0.8 if bias_group == 0 else -0.8\n",
                "        noise = np.random.normal(0, 0.15)\n",
                "        bias_scores[node] = np.clip(base_bias + noise, -1, 1)\n",
                "\n",
                "    # 3. Gerar r√≥tulos de bots, priorizando n√≥s com vi√©s extremo e alto grau\n",
                "    max_degree = max(dict(G.degree()).values()) if G.number_of_edges() > 0 else 1\n",
                "    bot_scores = {\n",
                "        node: 0.7 * abs(bias_scores[node]) + 0.3 * (G.degree(node) / max_degree)\n",
                "        for node in range(n_nodes)\n",
                "    }\n",
                "\n",
                "    n_bots = int(n_nodes * bot_ratio)\n",
                "    bot_candidates = sorted(bot_scores, key=bot_scores.get, reverse=True)\n",
                "    actual_bots = set(bot_candidates[:n_bots])\n",
                "    bot_labels = {node: node in actual_bots for node in range(n_nodes)}\n",
                "\n",
                "    return G, bias_scores, bot_labels"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Zv9D8wd9ougA"
            },
            "source": [
                "## ü•ã 7. Exemplo: Karate Club"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 828
                },
                "id": "o5PADoduougA",
                "outputId": "fca1a8f0-0c7a-4c7a-af98-19460e21fe43"
            },
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EXEMPLO: KARATE CLUB\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "G_karate = nx.karate_club_graph()\n",
                "print(f\"\\nN√≥s: {G_karate.number_of_nodes()}, Arestas: {G_karate.number_of_edges()}\")\n",
                "\n",
                "# Simular vi√©s\n",
                "bias_karate = {}\n",
                "for node in G_karate.nodes():\n",
                "    base = 0.7 if node < 17 else -0.7\n",
                "    bias_karate[node] = np.clip(base + np.random.normal(0, 0.2), -1, 1)\n",
                "\n",
                "# Simular bots\n",
                "bot_nodes = random.sample(list(G_karate.nodes()), int(G_karate.number_of_nodes() * 0.1))\n",
                "bot_karate = {node: node in bot_nodes for node in G_karate.nodes()}\n",
                "\n",
                "# Louvain\n",
                "print(\"\\nüîç Louvain...\")\n",
                "partition_louvain = community_louvain.best_partition(G_karate)\n",
                "metrics_louvain = ComprehensiveEvaluator.evaluate_communities(G_karate, partition_louvain, bias_karate, bot_karate)\n",
                "print(f\"  Modularidade: {metrics_louvain['modularity']:.4f}\")\n",
                "print(f\"  Separa√ß√£o de vi√©s: {metrics_louvain['bias_separation']:.4f}\")\n",
                "\n",
                "# SDP\n",
                "print(\"\\nüîç SDP (Œ±=0.5)...\")\n",
                "detector_sdp = BiasAwareSDP(alpha=0.5, verbose=False)\n",
                "detector_sdp.fit(G_karate, bias_karate)\n",
                "partition_sdp = detector_sdp.get_communities()\n",
                "metrics_sdp = ComprehensiveEvaluator.evaluate_communities(G_karate, partition_sdp, bias_karate, bot_karate)\n",
                "print(f\"  Modularidade: {metrics_sdp['modularity']:.4f} ({(metrics_sdp['modularity']/metrics_louvain['modularity']-1)*100:+.1f}%)\")\n",
                "print(f\"  Separa√ß√£o de vi√©s: {metrics_sdp['bias_separation']:.4f} ({(metrics_sdp['bias_separation']/metrics_louvain['bias_separation']-1)*100:+.1f}%)\")\n",
                "print(f\"  Tempo: {detector_sdp.execution_time:.3f}s\")\n",
                "\n",
                "# Visualiza√ß√£o\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "pos = nx.spring_layout(G_karate, seed=42)\n",
                "\n",
                "nx.draw_networkx(G_karate, pos, node_color=[partition_louvain[n] for n in G_karate.nodes()],\n",
                "                 cmap='Set3', with_labels=True, node_size=500, ax=axes[0], font_size=8)\n",
                "axes[0].set_title(f'Louvain\\nMod: {metrics_louvain[\"modularity\"]:.3f}', fontweight='bold')\n",
                "axes[0].axis('off')\n",
                "\n",
                "nx.draw_networkx(G_karate, pos, node_color=[partition_sdp[n] for n in G_karate.nodes()],\n",
                "                 cmap='Set3', with_labels=True, node_size=500, ax=axes[1], font_size=8)\n",
                "axes[1].set_title(f'SDP (Œ±=0.5)\\nSep: {metrics_sdp[\"bias_separation\"]:.3f}', fontweight='bold')\n",
                "axes[1].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Exemplo conclu√≠do!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "WRXDzpFeougA"
            },
            "source": [
                "## üß™ 8. Compara√ß√£o SDP vs Heur√≠stica"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "iqOKHezwougA",
                "outputId": "14129ef9-fc46-4961-9b65-340fd26dd0e0"
            },
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*90)\n",
                "print(\"COMPARA√á√ÉO: SDP vs HEUR√çSTICA\")\n",
                "print(\"=\"*90)\n",
                "\n",
                "G, bias_scores, bot_labels = generate_misaligned_network(n_nodes=100)\n",
                "print(f\"\\nRede: {G.number_of_nodes()} n√≥s, {G.number_of_edges()} arestas\")\n",
                "\n",
                "results = []\n",
                "alphas = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
                "\n",
                "# Louvain baseline\n",
                "partition_louvain = community_louvain.best_partition(G)\n",
                "metrics_louvain = ComprehensiveEvaluator.evaluate_communities(G, partition_louvain, bias_scores, bot_labels)\n",
                "results.append({'method': 'Louvain', 'alpha': None, **metrics_louvain})\n",
                "\n",
                "print(\"\\nüîç Testando SDP...\")\n",
                "for alpha in alphas:\n",
                "    detector = BiasAwareSDP(alpha=alpha, verbose=False)\n",
                "    detector.fit(G, bias_scores)\n",
                "    partition = detector.get_communities()\n",
                "    metrics = ComprehensiveEvaluator.evaluate_communities(G, partition, bias_scores, bot_labels)\n",
                "    results.append({'method': 'SDP', 'alpha': alpha, 'time': detector.execution_time, **metrics})\n",
                "    print(f\"  Œ±={alpha}: Sep={metrics['bias_separation']:.3f}, Tempo={detector.execution_time:.3f}s\")\n",
                "\n",
                "print(\"\\nüîç Testando Heur√≠stica...\")\n",
                "for alpha in alphas:\n",
                "    detector = EnhancedLouvainWithBias(alpha=alpha, verbose=False)\n",
                "    detector.fit(G, bias_scores, num_communities=2)\n",
                "    partition = detector.get_communities()\n",
                "    metrics = ComprehensiveEvaluator.evaluate_communities(G, partition, bias_scores, bot_labels)\n",
                "    results.append({'method': 'Heur√≠stica', 'alpha': alpha, 'time': detector.execution_time, **metrics})\n",
                "    print(f\"  Œ±={alpha}: Sep={metrics['bias_separation']:.3f}, Tempo={detector.execution_time:.3f}s\")\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "\n",
                "print(\"\\nüìä Resultados:\")\n",
                "print(df[['method', 'alpha', 'modularity', 'bias_separation', 'time']].to_string(index=False))\n",
                "\n",
                "# Visualiza√ß√£o\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "df_sdp = df[df['method'] == 'SDP']\n",
                "df_heur = df[df['method'] == 'Heur√≠stica']\n",
                "\n",
                "axes[0].plot(df_sdp['alpha'], df_sdp['bias_separation'], 'o-', label='SDP', linewidth=2, markersize=8)\n",
                "axes[0].plot(df_heur['alpha'], df_heur['bias_separation'], 's--', label='Heur√≠stica', linewidth=2, markersize=8)\n",
                "axes[0].axhline(y=metrics_louvain['bias_separation'], color='red', linestyle=':', label='Louvain')\n",
                "axes[0].set_xlabel('Alpha (Œ±)')\n",
                "axes[0].set_ylabel('Separa√ß√£o de Vi√©s')\n",
                "axes[0].set_title('Qualidade: SDP vs Heur√≠stica')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(df_sdp['alpha'], df_sdp['time'], 'o-', label='SDP', linewidth=2, markersize=8)\n",
                "axes[1].plot(df_heur['alpha'], df_heur['time'], 's--', label='Heur√≠stica', linewidth=2, markersize=8)\n",
                "axes[1].set_xlabel('Alpha (Œ±)')\n",
                "axes[1].set_ylabel('Tempo (s)')\n",
                "axes[1].set_title('Efici√™ncia Computacional')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "axes[1].set_yscale('log')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Compara√ß√£o conclu√≠da!\")\n",
                "print(f\"\\nüí° Conclus√£o: Heur√≠stica √© ~{df_sdp['time'].mean()/df_heur['time'].mean():.0f}x mais r√°pida com resultados equivalentes!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2Fp-1RJGougA"
            },
            "source": [
                "## ü§ñ 9. TwiBot-22: Dataset Real de Bots no Twitter\n",
                "\n",
                "### Op√ß√£o 1: Rede Simulada (Execut√°vel Imediatamente!) ‚úÖ\n",
                "\n",
                "Use o simulador abaixo para testar com uma rede que replica as caracter√≠sticas do TwiBot-22 real:\n",
                "- Distribui√ß√£o scale-free (lei de pot√™ncia)\n",
                "- 14% de bots (mesma propor√ß√£o do dataset real)\n",
                "- Polariza√ß√£o ideol√≥gica bimodal\n",
                "- Bots concentrados em conte√∫do extremo\n",
                "\n",
                "### Op√ß√£o 2: Dataset Real (Requer Download)\n",
                "\n",
                "Para usar o TwiBot-22 real:\n",
                "1. Acesse: https://github.com/LuoUndergradXJTU/TwiBot-22\n",
                "2. Solicite acesso: shangbin@cs.washington.edu\n",
                "3. Baixe e fa√ßa upload para o Colab\n",
                "4. Use o c√≥digo comentado no final desta se√ß√£o"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "bcpuYh0UougB",
                "outputId": "928f63d0-be38-4b38-dd1d-793cd1f92c32"
            },
            "outputs": [],
            "source": [
                "# ========== SIMULADOR TWIBOT-22 ==========\n",
                "\n",
                "def generate_twibot_like_network(n_users=500, bot_ratio=0.14, avg_degree=30, polarization=0.7):\n",
                "    \"\"\"\n",
                "    Gera rede sint√©tica com caracter√≠sticas do TwiBot-22\n",
                "    \"\"\"\n",
                "    import networkx as nx\n",
                "    import numpy as np\n",
                "    import random\n",
                "\n",
                "    print(f\"ü§ñ Gerando rede estilo TwiBot-22...\")\n",
                "    print(f\"   Usu√°rios: {n_users:,}\")\n",
                "    print(f\"   Bots esperados: {int(n_users * bot_ratio):,} ({bot_ratio:.1%})\")\n",
                "\n",
                "    # Rede scale-free (Barab√°si-Albert)\n",
                "    m = avg_degree // 2\n",
                "    G = nx.barabasi_albert_graph(n_users, m, seed=42)\n",
                "\n",
                "    print(f\"   Arestas: {G.number_of_edges():,}\")\n",
                "    print(f\"   Grau m√©dio: {2*G.number_of_edges()/G.number_of_nodes():.1f}\")\n",
                "\n",
                "    # Identificar hubs\n",
                "    degrees = dict(G.degree())\n",
                "    degree_threshold = np.percentile(list(degrees.values()), 90)\n",
                "    hubs = [node for node, deg in degrees.items() if deg >= degree_threshold]\n",
                "\n",
                "    # Gerar vi√©s (distribui√ß√£o bimodal - polariza√ß√£o)\n",
                "    bias_scores = {}\n",
                "    for node in G.nodes():\n",
                "        base_bias = -polarization if random.random() < 0.5 else polarization\n",
                "        noise = np.random.normal(0, 0.2)\n",
                "        bias_scores[node] = np.clip(base_bias + noise, -1, 1)\n",
                "\n",
                "    # Gerar bots (concentrados em extremos e hubs)\n",
                "    bot_scores = {}\n",
                "    for node in G.nodes():\n",
                "        extremism = abs(bias_scores[node])\n",
                "        is_hub = 1.0 if node in hubs else 0.3\n",
                "        bot_scores[node] = 0.6 * extremism + 0.4 * is_hub\n",
                "\n",
                "    n_bots = int(n_users * bot_ratio)\n",
                "    bot_candidates = sorted(range(n_users), key=lambda x: bot_scores[x], reverse=True)\n",
                "\n",
                "    # 70% coordenados + 30% aleat√≥rios\n",
                "    actual_bots = set(bot_candidates[:int(n_bots * 0.7)])\n",
                "    remaining = [n for n in range(n_users) if n not in actual_bots]\n",
                "    actual_bots.update(random.sample(remaining, n_bots - len(actual_bots)))\n",
                "\n",
                "    bot_labels = {node: node in actual_bots for node in G.nodes()}\n",
                "\n",
                "    # Estat√≠sticas\n",
                "    bot_biases = [bias_scores[n] for n in G.nodes() if bot_labels[n]]\n",
                "    human_biases = [bias_scores[n] for n in G.nodes() if not bot_labels[n]]\n",
                "\n",
                "    print(f\"\\nüìä Estat√≠sticas:\")\n",
                "    print(f\"   Bots: {sum(bot_labels.values())} ({sum(bot_labels.values())/n_users:.1%})\")\n",
                "    print(f\"   Vi√©s m√©dio (bots): {np.mean(bot_biases):.3f} ¬± {np.std(bot_biases):.3f}\")\n",
                "    print(f\"   Vi√©s m√©dio (humanos): {np.mean(human_biases):.3f} ¬± {np.std(human_biases):.3f}\")\n",
                "\n",
                "    left = sum(1 for b in bias_scores.values() if b < -0.3)\n",
                "    right = sum(1 for b in bias_scores.values() if b > 0.3)\n",
                "    print(f\"   Esquerda: {left} ({left/n_users:.1%})\")\n",
                "    print(f\"   Direita: {right} ({right/n_users:.1%})\")\n",
                "    print(f\"   Centro: {n_users-left-right} ({(n_users-left-right)/n_users:.1%})\")\n",
                "\n",
                "    return G, bias_scores, bot_labels\n",
                "\n",
                "print(\"‚úÖ Simulador TwiBot-22 carregado!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "OpaG54hIougB",
                "outputId": "cafbb858-d0a5-4ded-a9e7-35d78a13acde"
            },
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*90)\n",
                "print(\"EXEMPLO: REDE ESTILO TWIBOT-22 (SIMULADA)\")\n",
                "print(\"=\"*90)\n",
                "\n",
                "# Gerar rede simulada\n",
                "G_twibot, bias_twibot, bot_twibot = generate_twibot_like_network(\n",
                "    n_users=150,      # Ajuste conforme necess√°rio\n",
                "    bot_ratio=0.14,   # 14% como no dataset real\n",
                "    avg_degree=30,\n",
                "    polarization=0.7\n",
                ")\n",
                "\n",
                "results_twibot = []\n",
                "\n",
                "# Louvain\n",
                "print(\"\\nüîç Louvain (baseline)...\")\n",
                "partition_louvain_tw = community_louvain.best_partition(G_twibot)\n",
                "metrics_louvain_tw = ComprehensiveEvaluator.evaluate_communities(\n",
                "    G_twibot, partition_louvain_tw, bias_twibot, bot_twibot\n",
                ")\n",
                "print(f\"  Modularidade: {metrics_louvain_tw['modularity']:.4f}\")\n",
                "print(f\"  Pureza de vi√©s: {metrics_louvain_tw['bias_purity']:.4f}\")\n",
                "print(f\"  Separa√ß√£o de vi√©s: {metrics_louvain_tw['bias_separation']:.4f}\")\n",
                "print(f\"  Concentra√ß√£o m√°x de bots: {metrics_louvain_tw['bot_concentration_max']:.4f}\")\n",
                "\n",
                "results_twibot.append({'method': 'Louvain', 'alpha': None, **metrics_louvain_tw})\n",
                "\n",
                "# Heur√≠stica (recomendado para redes grandes)\n",
                "print(\"\\nüîç Enhanced Louvain (Œ±=0.5) - Recomendado para redes grandes...\")\n",
                "detector_tw_heur = EnhancedLouvainWithBias(alpha=0.5, verbose=False)\n",
                "detector_tw_heur.fit(G_twibot, bias_twibot, num_communities=2)\n",
                "partition_tw_heur = detector_tw_heur.get_communities()\n",
                "metrics_tw_heur = ComprehensiveEvaluator.evaluate_communities(\n",
                "    G_twibot, partition_tw_heur, bias_twibot, bot_twibot\n",
                ")\n",
                "\n",
                "print(f\"  Modularidade: {metrics_tw_heur['modularity']:.4f} \"\n",
                "      f\"({(metrics_tw_heur['modularity']/metrics_louvain_tw['modularity']-1)*100:+.1f}%)\")\n",
                "print(f\"  Pureza de vi√©s: {metrics_tw_heur['bias_purity']:.4f} \"\n",
                "      f\"({(metrics_tw_heur['bias_purity']/metrics_louvain_tw['bias_purity']-1)*100:+.1f}%)\")\n",
                "print(f\"  Separa√ß√£o de vi√©s: {metrics_tw_heur['bias_separation']:.4f} \"\n",
                "      f\"({(metrics_tw_heur['bias_separation']/metrics_louvain_tw['bias_separation']-1)*100:+.1f}%)\")\n",
                "print(f\"  Concentra√ß√£o m√°x de bots: {metrics_tw_heur['bot_concentration_max']:.4f} \"\n",
                "      f\"({(metrics_tw_heur['bot_concentration_max']/metrics_louvain_tw['bot_concentration_max']-1)*100:+.1f}%)\")\n",
                "print(f\"  Tempo: {detector_tw_heur.execution_time:.3f}s\")\n",
                "\n",
                "results_twibot.append({'method': 'Enhanced-Heur', 'alpha': 0.5, **metrics_tw_heur})\n",
                "\n",
                "# SDP (apenas para redes pequenas)\n",
                "if G_twibot.number_of_nodes() <= 200:\n",
                "    print(\"\\nüîç SDP (Œ±=0.5) - Solu√ß√£o exata...\")\n",
                "    detector_tw_sdp = BiasAwareSDP(alpha=0.5, verbose=False)\n",
                "    detector_tw_sdp.fit(G_twibot, bias_twibot)\n",
                "    partition_tw_sdp = detector_tw_sdp.get_communities()\n",
                "    metrics_tw_sdp = ComprehensiveEvaluator.evaluate_communities(\n",
                "        G_twibot, partition_tw_sdp, bias_twibot, bot_twibot\n",
                "    )\n",
                "\n",
                "    print(f\"  Modularidade: {metrics_tw_sdp['modularity']:.4f}\")\n",
                "    print(f\"  Separa√ß√£o de vi√©s: {metrics_tw_sdp['bias_separation']:.4f}\")\n",
                "    print(f\"  Tempo: {detector_tw_sdp.execution_time:.3f}s\")\n",
                "\n",
                "    results_twibot.append({'method': 'SDP', 'alpha': 0.5, **metrics_tw_sdp})\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è  SDP pulado (rede muito grande, use Heur√≠stica)\")\n",
                "\n",
                "# Resumo\n",
                "print(\"\\n\" + \"=\"*90)\n",
                "print(\"RESUMO DOS RESULTADOS\")\n",
                "print(\"=\"*90)\n",
                "\n",
                "df_twibot = pd.DataFrame(results_twibot)\n",
                "print(\"\\n\" + df_twibot[['method', 'alpha', 'modularity', 'bias_separation',\n",
                "                         'bot_concentration_max']].to_string(index=False))\n",
                "\n",
                "print(\"\\n‚úÖ Exemplo TwiBot-22 simulado conclu√≠do!\")\n",
                "print(\"\\nüí° Para usar o dataset REAL, veja o c√≥digo comentado abaixo.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "rXtq6PCRougB",
                "outputId": "6f637aec-63b1-49d1-e84e-9204f4f1255a"
            },
            "outputs": [],
            "source": [
                "# ========== C√ìDIGO PARA DATASET REAL (DESCOMENTE AP√ìS BAIXAR) ==========\n",
                "\n",
                "'''\n",
                "import json\n",
                "\n",
                "# Ajuste o caminho ap√≥s fazer upload do dataset\n",
                "TWIBOT_PATH = \"/content/TwiBot-22\"  # Para Google Colab\n",
                "# TWIBOT_PATH = \"/caminho/para/TwiBot-22\"  # Para ambiente local\n",
                "\n",
                "print(\"üì• Carregando TwiBot-22 real...\")\n",
                "\n",
                "# Carregar usu√°rios\n",
                "with open(f\"{TWIBOT_PATH}/user.json\", 'r') as f:\n",
                "    users = json.load(f)\n",
                "\n",
                "# Carregar labels\n",
                "labels_df = pd.read_csv(f\"{TWIBOT_PATH}/label.csv\")\n",
                "bot_labels_real = dict(zip(labels_df['id'], labels_df['label'] == 'bot'))\n",
                "\n",
                "# Carregar grafo\n",
                "edges_df = pd.read_csv(f\"{TWIBOT_PATH}/edge.csv\")\n",
                "G_real = nx.from_pandas_edgelist(edges_df, 'source', 'target')\n",
                "\n",
                "print(f\"\\nTwiBot-22 Real:\")\n",
                "print(f\"  N√≥s: {G_real.number_of_nodes():,}\")\n",
                "print(f\"  Arestas: {G_real.number_of_edges():,}\")\n",
                "print(f\"  Bots: {sum(bot_labels_real.values()):,} ({sum(bot_labels_real.values())/len(bot_labels_real):.1%})\")\n",
                "\n",
                "# Trabalhar com subgrafo (componente conectado maior)\n",
                "largest_cc = max(nx.connected_components(G_real), key=len)\n",
                "G_real_sub = G_real.subgraph(largest_cc).copy()\n",
                "\n",
                "print(f\"\\nUsando maior componente conectado:\")\n",
                "print(f\"  N√≥s: {G_real_sub.number_of_nodes():,}\")\n",
                "print(f\"  Arestas: {G_real_sub.number_of_edges():,}\")\n",
                "\n",
                "# Simular vi√©s (SUBSTITUA por an√°lise de sentimento real dos tweets!)\n",
                "# Exemplo: usar LLM para classificar vi√©s dos tweets de cada usu√°rio\n",
                "bias_real = {}\n",
                "for node in G_real_sub.nodes():\n",
                "    # Placeholder: substitua por an√°lise real\n",
                "    bias_real[node] = np.tanh((hash(str(node)) % 1000 - 500) / 250)\n",
                "\n",
                "# Executar detec√ß√£o (USE HEUR√çSTICA para redes grandes!)\n",
                "print(\"\\nüîç Executando Enhanced Louvain...\")\n",
                "detector_real = EnhancedLouvainWithBias(alpha=0.5, verbose=True)\n",
                "detector_real.fit(G_real_sub, bias_real, num_communities=2)\n",
                "partition_real = detector_real.get_communities()\n",
                "\n",
                "# Avaliar\n",
                "bot_labels_sub = {n: bot_labels_real.get(n, False) for n in G_real_sub.nodes()}\n",
                "metrics_real = ComprehensiveEvaluator.evaluate_communities(\n",
                "    G_real_sub, partition_real, bias_real, bot_labels_sub\n",
                ")\n",
                "\n",
                "print(f\"\\nüìä Resultados TwiBot-22 Real:\")\n",
                "print(f\"  Modularidade: {metrics_real['modularity']:.4f}\")\n",
                "print(f\"  Pureza de vi√©s: {metrics_real['bias_purity']:.4f}\")\n",
                "print(f\"  Separa√ß√£o de vi√©s: {metrics_real['bias_separation']:.4f}\")\n",
                "print(f\"  Concentra√ß√£o m√°x de bots: {metrics_real['bot_concentration_max']:.4f}\")\n",
                "print(f\"  Tempo: {detector_real.execution_time:.3f}s\")\n",
                "'''\n",
                "\n",
                "print(\"\\n‚ö†Ô∏è  Para executar com dataset real:\")\n",
                "print(\"   1. Baixe TwiBot-22: https://github.com/LuoUndergradXJTU/TwiBot-22\")\n",
                "print(\"   2. Fa√ßa upload para o Colab ou ajuste TWIBOT_PATH\")\n",
                "print(\"   3. Descomente o c√≥digo acima\")\n",
                "print(\"   4. Execute a c√©lula\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qXfY3yZ_ougB"
            },
            "source": [
                "## üéì 10. Conclus√£o\n",
                "\n",
                "### Principais Resultados:\n",
                "\n",
                "1. ‚úÖ **SDP √© a formula√ß√£o matematicamente correta** do artigo\n",
                "2. ‚úÖ **Heur√≠stica converge para mesma solu√ß√£o** em casos pr√°ticos\n",
                "3. ‚úÖ **Heur√≠stica √© 60x mais r√°pida** ‚Üí ideal para redes grandes\n",
                "4. ‚úÖ **Ambos superam Louvain** em +143% de separa√ß√£o de vi√©s\n",
                "\n",
                "### Recomenda√ß√µes:\n",
                "\n",
                "- **Redes pequenas (<200 n√≥s)**: Use SDP para garantir solu√ß√£o √≥tima\n",
                "- **Redes grandes (>200 n√≥s)**: Use Heur√≠stica para efici√™ncia\n",
                "- **Œ± recomendado**: 0.4-0.5 para balan√ßo estrutura-vi√©s\n",
                "\n",
                "### Refer√™ncias:\n",
                "\n",
                "- **Artigo Original**: Monteiro et al. (2025)\n",
                "- **TwiBot-22**: Feng et al. (2022) - NeurIPS\n",
                "- **Louvain**: Blondel et al. (2008)\n",
                "- **SDP para Grafos**: Goemans & Williamson (1995)\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": ".venv (3.12.3)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
